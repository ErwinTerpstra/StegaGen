{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52220c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf2aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA on NVIDIA RTX 500 Ada Generation Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    device_name = torch.cuda.get_device_name(device)\n",
    "\n",
    "    print(f'CUDA on {device_name}')\n",
    "else:\n",
    "    print(f'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd88ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_LENGTH = 16\n",
    "MAX_STEGO_LENGTH = 256\n",
    "\n",
    "# Using BF16 seems to fix some inf/nan errors in one of the models\n",
    "dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9584927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Delete models, clean up memory\n",
    "\n",
    "if 'generator_tokenizer' in globals():\n",
    "    del generator_tokenizer\n",
    "    \n",
    "if 'decoder_tokenizer' in globals():\n",
    "    del decoder_tokenizer\n",
    "\n",
    "if 'generator' in globals():\n",
    "    del generator\n",
    "    \n",
    "if 'decoder' in globals():\n",
    "    del decoder\n",
    "    \n",
    "if 'semantic_anchor' in globals():\n",
    "    del semantic_anchor\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch._C._cuda_clearCublasWorkspaces()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef47b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.0 MB; Reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Check if everything's cleaned\n",
    "allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
    "reserved = torch.cuda.memory_reserved() / (1024 * 1024)\n",
    "\n",
    "print(f'Allocated: {allocated:.1f} MB; Reserved: {reserved:.1f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbad1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddingDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size=768, sequence_length=16):\n",
    "        super().__init__()\n",
    "\n",
    "        config = BertConfig(\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=4,\n",
    "            num_attention_heads=8,\n",
    "            intermediate_size=4 * hidden_size,\n",
    "            max_position_embeddings=sequence_length,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "        self.bert = BertModel(config).to(device)\n",
    "\n",
    "        # Token-wise binary classification head\n",
    "        self.classifier = nn.Linear(hidden_size, 1, device=device, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, inputs_embeds, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        inputs_embeds: (B, N, D)\n",
    "        attention_mask: (B, N) optional\n",
    "        labels: (B, N) optional\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.bert(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # (B, N, D)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # (B, N, 1)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(logits.squeeze(-1), labels.float())\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed32d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding head that uses hidden states of an LM network to perform binary classification\n",
    "# instead of outputting text\n",
    "class BitDecoder(nn.Module):\n",
    "    def __init__(self, decoder_model, decoder_pad_token, bit_len):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder_model\n",
    "        self.decoder_pad_token = decoder_pad_token\n",
    "        self.bit_len = bit_len\n",
    "\n",
    "        self.bit_head = nn.Linear(\n",
    "            decoder_model.config.hidden_size,\n",
    "            2,  # Binary classification, so one logit per value\n",
    "            dtype=dtype\n",
    "        ).to(device)\n",
    "        \n",
    "        self.decoder_input_ids = torch.full(\n",
    "            (1, self.bit_len),\n",
    "            self.decoder_pad_token,\n",
    "            dtype=torch.long\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        outputs = self.decoder(\n",
    "            **kwargs,\n",
    "            decoder_input_ids=self.decoder_input_ids,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        # Use last hidden state from LM\n",
    "        h = outputs.decoder_hidden_states[-1]  # [B, T, d]\n",
    "\n",
    "        # Pool or select fixed positions\n",
    "        h_bits = h[:, :self.bit_len, :]  # [B, bit_len, d]\n",
    "\n",
    "        logits = self.bit_head(h_bits)   # [B, bit_len, 2]\n",
    "        return logits\n",
    "    \n",
    "    def decode(self, inputs):\n",
    "        # Decode\n",
    "        bit_logits = self(input_ids=inputs)\n",
    "        bits = bit_logits.argmax(dim=-1)\n",
    "\n",
    "        return bits[0, :self.bit_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a15a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GENERATOR_MODEL = \"Qwen/Qwen3-0.6B\"\n",
    "#GENERATOR_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "#GENERATOR_MODEL = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "GENERATOR_MODEL = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "#DECODER_MODEL = \"google/flan-t5-base\"\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL)\n",
    "#decoder_tokenizer = T5Tokenizer.from_pretrained(DECODER_MODEL)\n",
    "\n",
    "generator = AutoModelForCausalLM.from_pretrained(GENERATOR_MODEL, dtype=dtype).to(device)\n",
    "decoder = BertEmbeddingDecoder(hidden_size=generator.config.hidden_size, sequence_length=SECRET_LENGTH)\n",
    "#decoder = T5ForConditionalGeneration.from_pretrained(DECODER_MODEL, dtype=dtype).to(device)\n",
    "\n",
    "semantic_anchor = AutoModelForCausalLM.from_pretrained(GENERATOR_MODEL, dtype=dtype).to(device)\n",
    "\n",
    "# Freeze base model\n",
    "for p in semantic_anchor.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "generator.train()\n",
    "decoder.train()\n",
    "semantic_anchor.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf864564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojection layer to map generator to decoder embeddings\n",
    "#proj_layer = torch.nn.Linear(generator.config.hidden_size, decoder.config.hidden_size, device=device, dtype=dtype)\n",
    "\n",
    "# Bit decoder will map decoder logits to binary secret\n",
    "#bit_decoder = BitDecoder(decoder, decoder_tokenizer.pad_token_type_id, SECRET_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110fecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator_prompt(carrier, secret_str):\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{carrier}. Secret: {secret_str}\"}]\n",
    "    return generator_tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da5da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_forward(temperature=1.0, **kwargs):\n",
    "    out = generator(\n",
    "        **kwargs,\n",
    "        return_dict=True,\n",
    "        use_cache=True,\n",
    "        eos_token_id=None, # prevent early stop\n",
    "    )\n",
    "\n",
    "    logits = out.logits[:, -1, :]   # [B, V]\n",
    "    probs  = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "    return logits, probs, out.past_key_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97fe035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_embeddings(token_probs):\n",
    "    embed_matrix = generator.get_input_embeddings().weight  # [V, d]\n",
    "    return token_probs @ embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8db77fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def anchor_forward(anchor, inputs) -> torch.Tensor:\n",
    "    outputs = anchor(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        return_dict=True\n",
    "    )\n",
    "    logits = outputs.logits[:, :SECRET_LENGTH, :]\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8246cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_semantic_loss(stego_logits, base_logits):\n",
    "    p = F.log_softmax(stego_logits, dim=-1)\n",
    "    q = F.softmax(base_logits, dim=-1)\n",
    "\n",
    "    # Masked?\n",
    "    # kl = F.kl_div(log_p, q, reduction=\"none\")  # [B, T, V]\n",
    "    # kl = kl.sum(dim=-1)                        # [B, T]\n",
    "\n",
    "    # # Mask padding tokens\n",
    "    # kl = kl * attention_mask\n",
    "\n",
    "    # return kl.sum() / attention_mask.sum()\n",
    "\n",
    "    return F.kl_div(p, q, reduction=\"batchmean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f42accc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bit_loss(logits, bits):\n",
    "    \"\"\"\n",
    "    logits: [B, bit_len, 2]\n",
    "    bits:   [B, bit_len]  (0/1)\n",
    "    \"\"\"\n",
    "    return F.cross_entropy(\n",
    "        logits.view(-1, 2),\n",
    "        bits.view(-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ccc2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_forward(soft_embeds, secret_bits):\n",
    "    out = decoder(inputs_embeds=soft_embeds, labels=secret_bits)\n",
    "\n",
    "    #loss = bit_loss(logits, secret_bits)\n",
    "\n",
    "    return out['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f60c8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode(carrier, secret):\n",
    "    prompt = build_generator_prompt(carrier, secret)\n",
    "    inputs = generator_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    ids = generator.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=SECRET_LENGTH + 1, # +1 seems necessary somehow\n",
    "        eos_token_id=None, # prevent early stop\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    print(ids.shape)\n",
    "    return generator_tokenizer.decode(ids[0, -SECRET_LENGTH:], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a640af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def decode(stego_text):\n",
    "    encoded_token_ids = generator_tokenizer(stego_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Encoder embedding matrix\n",
    "    embedding_layer = generator.get_input_embeddings()\n",
    "\n",
    "    # (B, N, D)\n",
    "    encoded_embeds = embedding_layer(encoded_token_ids).to(device)\n",
    "\n",
    "    #attention_mask = torch.ones(encoded_embeds.shape[:2])\n",
    "\n",
    "    out = decoder(\n",
    "        inputs_embeds=encoded_embeds\n",
    "        #attention_mask=attention_mask,\n",
    "    )\n",
    "\n",
    "    logits = out[\"logits\"]      # (B, N, 1)\n",
    "    preds = (logits.squeeze(-1) > 0).long()\n",
    "    \n",
    "    return preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f66358a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_to_str(bits):\n",
    "    return \"\".join(str(b) for b in bits.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3090283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 68])\n",
      "Step 0 | L_sem: 0.000 | L_rec: 0.770\n",
      "Secret:  1000001100110111\n",
      "Decoded: 1101110011101000\n",
      "Stego:   assistant\n",
      "Neural networks learn by iteratively updating the weights and biases of\n",
      "\n",
      "torch.Size([1, 68])\n",
      "Step 1 | L_sem: 0.000 | L_rec: 0.747\n",
      "Secret:  0000001000101110\n",
      "Decoded: 0100010011101000\n",
      "Stego:   assistant\n",
      "Neural networks learn by iteratively updating the weights and biases of\n",
      "\n",
      "torch.Size([1, 68])\n",
      "Step 2 | L_sem: 0.000 | L_rec: 0.589\n",
      "Secret:  0000101001001101\n",
      "Decoded: 0000010001100000\n",
      "Stego:   assistant\n",
      "Neural networks learn by iteratively updating the weights and biases of\n",
      "\n",
      "torch.Size([1, 68])\n",
      "Step 3 | L_sem: 0.000 | L_rec: 0.723\n",
      "Secret:  1010111011001111\n",
      "Decoded: 0000010001100000\n",
      "Stego:   assistant\n",
      "Neural networks learn by iteratively updating the weights and biases of\n",
      "\n",
      "torch.Size([1, 68])\n",
      "Step 4 | L_sem: 0.000 | L_rec: 0.737\n",
      "Secret:  0011101111000101\n",
      "Decoded: 0000010011101000\n",
      "Stego:   assistant\n",
      "Neural networks learn by iteratively updating the weights and biases of\n",
      "\n",
      "torch.Size([1, 68])\n",
      "Step 5 | L_sem: 0.000 | L_rec: 0.775\n",
      "Secret:  0100011011111010\n",
      "Decoded: 0101011011101000\n",
      "Stego:   assistant\n",
      "Neural networks learn by iteratively updating the weights and biases of\n",
      "\n",
      "torch.Size([1, 68])\n",
      "Step 6 | L_sem: 0.000 | L_rec: 0.780\n",
      "Secret:  0101011111110011\n",
      "Decoded: 0101010011101010\n",
      "Stego:   assistant\n",
      "Neural networks learn by iteratively updating the weights and biases of\n",
      "\n",
      "torch.Size([1, 68])\n",
      "Step 7 | L_sem: 0.000 | L_rec: 0.667\n",
      "Secret:  1010101010000110\n",
      "Decoded: 0001010011101010\n",
      "Stego:   assistant\n",
      "Neural networks learn by iteratively updating the weights and biases of\n",
      "\n",
      "torch.Size([1, 68])\n",
      "Step 8 | L_sem: 0.000 | L_rec: 0.883\n",
      "Secret:  0101100001001111\n",
      "Decoded: 0001011111101110\n",
      "Stego:   assistant\n",
      "Neural networks learn by iteratively updating the weights and biases of\n",
      "\n",
      "torch.Size([1, 68])\n",
      "Step 9 | L_sem: 0.000 | L_rec: 0.592\n",
      "Secret:  0001111001101111\n",
      "Decoded: 0001010111111110\n",
      "Stego:   assistant\n",
      "Neural networks learn by iteratively updating the weights and biases of\n",
      "\n",
      "torch.Size([1, 68])\n",
      "Step 10 | L_sem: 0.000 | L_rec: 0.909\n",
      "Secret:  0010000100111100\n",
      "Decoded: 0011010111111101\n",
      "Stego:   assistant\n",
      "Neural networks learn by iteratively updating the weights and biases of\n",
      "\n",
      "torch.Size([1, 68])\n",
      "Step 11 | L_sem: 0.000 | L_rec: 0.820\n",
      "Secret:  0111110101101110\n",
      "Decoded: 0011010111111101\n",
      "Stego:   assistant\n",
      "Neural networks learn by iteratively updating the weights and biases of\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m loss = recovery_loss\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m#loss = 1.0 * recovery_loss + 0.0 * semantic_loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m optimizer.step()\n\u001b[32m     51\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    list(generator.parameters()) + \n",
    "    list(decoder.parameters()),\n",
    "    #list(proj_layer.parameters()) + \n",
    "    #list(bit_decoder.parameters()),\n",
    "    lr=3e-5\n",
    ")\n",
    "\n",
    "for step in range(1000):\n",
    "    carrier_prompt = \"Explain how neural networks learn.\"\n",
    "    secret_bits = torch.randint(0, 2, (1, SECRET_LENGTH))   # [ B, N ]\n",
    "    secret_str = bits_to_str(secret_bits.flatten())\n",
    "\n",
    "    prompt = build_generator_prompt(carrier_prompt, secret_str)\n",
    "    generator_inputs = generator_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    past_kv = None\n",
    "\n",
    "    # Generate N tokens, store embeddings for each token\n",
    "    generated_embeds = []\n",
    "    for t in range(SECRET_LENGTH):\n",
    "        stego_logits, token_probs, past_kv = generator_forward(\n",
    "            input_ids=generator_inputs.input_ids if t == 0 else None,\n",
    "            inputs_embeds=soft_embeds.unsqueeze(1) if t > 0 else None,\n",
    "            past_key_values=past_kv\n",
    "        )\n",
    "\n",
    "        soft_embeds = soft_embeddings(token_probs)  # [ B, D ]\n",
    "        generated_embeds.append(soft_embeds)\n",
    "\n",
    "    # Stack embeddings for each token to [ B, N, D ]\n",
    "    encoder_outputs_embeds = torch.stack(generated_embeds, dim=1).to(device)\n",
    "\n",
    "    # Linear projection to decoder embedding space\n",
    "    #decoder_inputs = proj_layer(soft_embeds)     # [B, T, d_dec]\n",
    "\n",
    "    # Calculate recovery loss based on decoding success\n",
    "    recovery_loss = decoder_forward(encoder_outputs_embeds, secret_bits.to(device))\n",
    "\n",
    "    # Calculate semantic loss compared to base model\n",
    "    #base_logits = anchor_forward(semantic_anchor, generator_inputs)\n",
    "    #semantic_loss = kl_semantic_loss(stego_logits, base_logits)\n",
    "    semantic_loss = np.zeros(1)\n",
    "\n",
    "    # Combine loss\n",
    "    loss = recovery_loss\n",
    "    #loss = 1.0 * recovery_loss + 0.0 * semantic_loss\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 1 == 0:\n",
    "        stego_text = encode(carrier_prompt, secret_str)\n",
    "        decoded_secret_bits = decode(stego_text)\n",
    "        decoded_secret_str = bits_to_str(decoded_secret_bits)\n",
    "\n",
    "        print(f\"Step {step} | L_sem: {semantic_loss.item():.3f} | L_rec: {recovery_loss.item():.3f}\")\n",
    "        print(f'Secret:  {secret_str}')\n",
    "        print(f'Decoded: {decoded_secret_str}')\n",
    "        print(f'Stego:   {stego_text}')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb69447",
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier = \"Write a short paragraph explaining how neural networks learn.\"\n",
    "secret = \"1011001110001111\"\n",
    "\n",
    "stego_text = encode(carrier, secret)\n",
    "recovered_secret = decode(stego_text)\n",
    "\n",
    "print(\"Stego text:\")\n",
    "print(stego_text)\n",
    "print()\n",
    "print(\"Original secret:\", secret)\n",
    "print(\"Recovered secret:\", bits_to_str(recovered_secret))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stega",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
