{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52220c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf2aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA on NVIDIA RTX 500 Ada Generation Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    device_name = torch.cuda.get_device_name(device)\n",
    "\n",
    "    print(f'CUDA on {device_name}')\n",
    "else:\n",
    "    print(f'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd88ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_LENGTH = 16\n",
    "TOKENS_PER_BIT = 1\n",
    "SKIPPED_OUTPUT_TOKENS = 4   # Number of initial output tokens to skip (seem to be constant for SmolLM)\n",
    "PREFIX_LENGTH = 32          # Number of tokens in which the secret is encoded before the input prompt\n",
    "\n",
    "STEGO_LENGTH = SECRET_LENGTH * TOKENS_PER_BIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9584927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Delete models, clean up memory\n",
    "\n",
    "if 'generator_tokenizer' in globals():\n",
    "    del generator_tokenizer\n",
    "    \n",
    "if 'decoder_tokenizer' in globals():\n",
    "    del decoder_tokenizer\n",
    "\n",
    "if 'generator' in globals():\n",
    "    del generator\n",
    "    \n",
    "if 'decoder' in globals():\n",
    "    del decoder\n",
    "    \n",
    "if 'semantic_anchor' in globals():\n",
    "    del semantic_anchor\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch._C._cuda_clearCublasWorkspaces()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef47b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.0 MB; Reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Check if everything's cleaned\n",
    "allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
    "reserved = torch.cuda.memory_reserved() / (1024 * 1024)\n",
    "\n",
    "print(f'Allocated: {allocated:.1f} MB; Reserved: {reserved:.1f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbad1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StegoEmbeddingDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size=768, output_size=16, pool_size=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "        # config = BertConfig(\n",
    "        #     hidden_size=hidden_size,\n",
    "        #     num_hidden_layers=4,\n",
    "        #     num_attention_heads=8,\n",
    "        #     intermediate_size=4 * hidden_size\n",
    "        # )\n",
    "\n",
    "        # self.bert = BertModel(config)\n",
    "\n",
    "        # Token-wise binary classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * pool_size, 256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs_embeds, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        inputs_embeds: (B, N, D)\n",
    "        attention_mask: (B, N) optional\n",
    "        labels: (B, N) optional\n",
    "        \"\"\"\n",
    "\n",
    "        # outputs = self.bert(\n",
    "        #     inputs_embeds=inputs_embeds,\n",
    "        #     attention_mask=attention_mask,\n",
    "        #     return_dict=True,\n",
    "        # )\n",
    "\n",
    "        # hidden_states = outputs.last_hidden_state # (B, N, D)\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # Reshape embeddings per token to stack all embeddings for a bit pool\n",
    "        assert hidden_states.shape[1] == self.output_size * self.pool_size, f'Input tokens size error: {hidden_states.shape[1]}. Expected: {self.output_size * self.pool_size}'\n",
    "        assert hidden_states.shape[2] == self.hidden_size, f' Input embedding size error: {hidden_states.shape[2]}. Expected: {self.hidden_size}'\n",
    "        classifier_input = hidden_states.reshape(-1, self.output_size, self.pool_size * self.hidden_size)  # (B, T, P * D)\n",
    "        \n",
    "        # Perform classification to get logits for each bit\n",
    "        logits = self.classifier(classifier_input)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(logits.squeeze(-1), labels.float())\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3e9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SecretPrefixEncoder(nn.Module):\n",
    "    def __init__(self, bit_len: int, hidden_size: int, prefix_length: int):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.hidden_size = hidden_size\n",
    "        self.prefix_length = prefix_length\n",
    "\n",
    "        layer_size = hidden_size * prefix_length\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(bit_len, layer_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(layer_size, layer_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, bits: torch.Tensor):\n",
    "        \"\"\"\n",
    "        bits: [ B, N ] in {0,1}\n",
    "        returns: [ B, P, D ]\n",
    "        \"\"\"\n",
    "        emb = self.projection(bits.float())\n",
    "        return emb.reshape(bits.shape[0], self.prefix_length, self.hidden_size) # [ B, P, D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a15a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator embedding dimensions: 576\n"
     ]
    }
   ],
   "source": [
    "#GENERATOR_MODEL = \"Qwen/Qwen3-0.6B\"\n",
    "#GENERATOR_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "#GENERATOR_MODEL = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "GENERATOR_MODEL = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "#DECODER_MODEL = \"google/flan-t5-base\"\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL)\n",
    "#decoder_tokenizer = T5Tokenizer.from_pretrained(DECODER_MODEL).to(device)\n",
    "\n",
    "generator = AutoModelForCausalLM.from_pretrained(GENERATOR_MODEL).to(device)\n",
    "decoder = StegoEmbeddingDecoder(hidden_size=generator.config.hidden_size, output_size=SECRET_LENGTH, pool_size=TOKENS_PER_BIT).to(device)\n",
    "#decoder = T5ForConditionalGeneration.from_pretrained(DECODER_MODEL).to(device)\n",
    "\n",
    "prefix_encoder = SecretPrefixEncoder(bit_len=SECRET_LENGTH, hidden_size=generator.config.hidden_size, prefix_length=PREFIX_LENGTH).to(device)\n",
    "\n",
    "#semantic_anchor = AutoModelForCausalLM.from_pretrained(GENERATOR_MODEL).to(device)\n",
    "\n",
    "# Freeze base model\n",
    "#for p in semantic_anchor.parameters():\n",
    "#    p.requires_grad = False\n",
    "\n",
    "generator.train()\n",
    "decoder.train()\n",
    "#semantic_anchor.eval()\n",
    "\n",
    "print(f'Generator embedding dimensions: {generator.config.hidden_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "110fecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_to_str(bits, sep=''):\n",
    "    return sep.join(str(b) for b in bits.tolist())\n",
    "\n",
    "def build_generator_prompt(carrier, secret_bits):\n",
    "    # secret_prompt = f' Encode these secret bits in your answer: {bits_to_str(secret_bits.flatten(), \" \")}'\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\"}, \n",
    "        {\"role\": \"user\", \"content\": carrier }\n",
    "    ]\n",
    "\n",
    "    return generator_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "def generate_input_embeddings(input_ids, secret_bits):\n",
    "    # Token embeddings\n",
    "    token_embeds = generator.get_input_embeddings()(input_ids) # (B, T, D)\n",
    "\n",
    "    # Prefix embeddings\n",
    "    prefix_embeds = prefix_encoder(secret_bits)\n",
    "\n",
    "    # Concatenate prefix and token embeddings\n",
    "    input_embeds = torch.cat([prefix_embeds, token_embeds], dim=1)  # (B, T+1, D)\n",
    "    \n",
    "    return input_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db77fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_forward(temperature=1.0, **kwargs):\n",
    "    out = generator(\n",
    "        **kwargs,\n",
    "        return_dict=True,\n",
    "        use_cache=True,\n",
    "        eos_token_id=None, # prevent early stop\n",
    "    )\n",
    "\n",
    "    logits = out.logits[:, -1, :]   # [B, V]\n",
    "    probs  = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "    return logits, probs, out.past_key_values\n",
    "\n",
    "def soft_embeddings(token_probs):\n",
    "    embed_matrix = generator.get_input_embeddings().weight  # [V, d]\n",
    "    return token_probs @ embed_matrix\n",
    "\n",
    "@torch.no_grad()\n",
    "def anchor_forward(anchor, inputs) -> torch.Tensor:\n",
    "    outputs = anchor(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        return_dict=True\n",
    "    )\n",
    "    logits = outputs.logits[:, :STEGO_LENGTH, :]\n",
    "    return logits\n",
    "\n",
    "def kl_semantic_loss(stego_logits, base_logits):\n",
    "    p = F.log_softmax(stego_logits, dim=-1)\n",
    "    q = F.softmax(base_logits, dim=-1)\n",
    "\n",
    "    # Masked?\n",
    "    # kl = F.kl_div(log_p, q, reduction=\"none\")  # [B, T, V]\n",
    "    # kl = kl.sum(dim=-1)                        # [B, T]\n",
    "\n",
    "    # # Mask padding tokens\n",
    "    # kl = kl * attention_mask\n",
    "\n",
    "    # return kl.sum() / attention_mask.sum()\n",
    "\n",
    "    return F.kl_div(p, q, reduction=\"batchmean\")\n",
    "\n",
    "def decoder_forward(soft_embeds, secret_bits):\n",
    "    out = decoder(inputs_embeds=soft_embeds, labels=secret_bits)\n",
    "\n",
    "    return out['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f60c8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode(carrier, secret_bits):\n",
    "    prompt = build_generator_prompt(carrier, secret_bits)\n",
    "    inputs = generator_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_embeddings = generate_input_embeddings(inputs.input_ids.to(device), secret_bits)\n",
    "\n",
    "    ids = generator.generate(\n",
    "        inputs_embeds=input_embeddings,\n",
    "        max_new_tokens=STEGO_LENGTH + SKIPPED_OUTPUT_TOKENS, # First X tokens seem to be constant part of the LMs output\n",
    "        eos_token_id=None, # prevent early stop\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    selected_ids = ids[0, SKIPPED_OUTPUT_TOKENS:]\n",
    "\n",
    "    assert selected_ids.shape[0] == STEGO_LENGTH, f'Generated stego length error: {selected_ids.shape[0]}. Expected: {STEGO_LENGTH}'\n",
    "\n",
    "    return generator_tokenizer.decode(selected_ids, skip_special_tokens=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode(stego_text):\n",
    "    encoded_token_ids = generator_tokenizer(stego_text, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    # Ensure correct length, discard extra tokens and pad if too short\n",
    "    encoded_token_ids = encoded_token_ids[:, :STEGO_LENGTH]\n",
    "    encoded_token_ids = F.pad(encoded_token_ids, (0, STEGO_LENGTH - encoded_token_ids.shape[1]), value=generator_tokenizer.pad_token_id)\n",
    "\n",
    "    # Encoder embedding matrix\n",
    "    embedding_layer = generator.get_input_embeddings()\n",
    "\n",
    "    # (B, N, D)\n",
    "    encoded_embeds = embedding_layer(encoded_token_ids.to(device))\n",
    "\n",
    "    #attention_mask = torch.ones(encoded_embeds.shape[:2])\n",
    "\n",
    "    out = decoder(\n",
    "        inputs_embeds=encoded_embeds\n",
    "        #attention_mask=attention_mask,\n",
    "    )\n",
    "\n",
    "    logits = out[\"logits\"]      # (B, N, 1)\n",
    "    preds = (logits.squeeze(-1) > 0).long()\n",
    "    \n",
    "    return preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94de9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_secret():\n",
    "    return torch.randint(0, 2, (1, SECRET_LENGTH))   # [ B, N ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da514610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Write a short paragraph explaining how neural networks learn.<|im_end|>\n",
      "\n",
      "----------------\n",
      "Neural networks learn by training on a dataset of labeled data, where each instance\n"
     ]
    }
   ],
   "source": [
    "carrier = \"Write a short paragraph explaining how neural networks learn.\"\n",
    "secret = generate_secret().to(device)\n",
    "\n",
    "print(build_generator_prompt(carrier, secret))\n",
    "print('----------------')\n",
    "print(encode(carrier, secret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3090283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | L_sem: 0.000 | L_rec: 0.707\n",
      "Secret:  1011110111110000\n",
      "Decoded: 0000000000000000\n",
      "Stego:   \n",
      "\n",
      "Neural networks learn by processing data through the combination of inputs and outputs\n",
      "\n",
      "Step 1 | L_sem: 0.000 | L_rec: 0.683\n",
      "Secret:  0100000001111001\n",
      "Decoded: 0000000000000000\n",
      "Stego:   Neural networks learn by processing and transforming data into patterns, which they then use\n",
      "\n",
      "Step 2 | L_sem: 0.000 | L_rec: 0.710\n",
      "Secret:  0110011110111101\n",
      "Decoded: 0000000000000000\n",
      "Stego:   system\n",
      "system\n",
      "\n",
      "user\n",
      "You I\n",
      "\n",
      "Step 3 | L_sem: 0.000 | L_rec: 0.694\n",
      "Secret:  1010101001010110\n",
      "Decoded: 0000000000000000\n",
      "Stego:   \n",
      "\n",
      "\n",
      "Neural networks learn by processing data from the input data and transforming\n",
      "\n",
      "Step 4 | L_sem: 0.000 | L_rec: 0.685\n",
      "Secret:  1010100110010000\n",
      "Decoded: 0000000000000000\n",
      "Stego:   \n",
      "\n",
      "\n",
      "\n",
      "Neural networks learn by processing data and using algorithms to identify\n",
      "\n",
      "Step 5 | L_sem: 0.000 | L_rec: 0.703\n",
      "Secret:  0111001011001111\n",
      "Decoded: 0000000000000000\n",
      "Stego:   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Step 6 | L_sem: 0.000 | L_rec: 0.693\n",
      "Secret:  0010100111110010\n",
      "Decoded: 0000000000000000\n",
      "Stego:   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Step 7 | L_sem: 0.000 | L_rec: 0.693\n",
      "Secret:  1101101100001001\n",
      "Decoded: 0000000000000000\n",
      "Stego:   system\n",
      "system\n",
      "user\n",
      "system\n",
      "\n",
      "\n",
      "Step 8 | L_sem: 0.000 | L_rec: 0.677\n",
      "Secret:  0000100101010000\n",
      "Decoded: 0000000000000000\n",
      "Stego:   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \"ArynznampaNamp\n",
      "\n",
      "Step 9 | L_sem: 0.000 | L_rec: 0.689\n",
      "Secret:  0001101110100010\n",
      "Decoded: 0000000000000000\n",
      "Stego:   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " oneEte\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Step 10 | L_sem: 0.000 | L_rec: 0.693\n",
      "Secret:  0011011001010110\n",
      "Decoded: 0000000000000000\n",
      "Stego:   ��E�E�E�E�EE�E�E\n",
      "\n",
      "Step 11 | L_sem: 0.000 | L_rec: 0.705\n",
      "Secret:  0111101110011101\n",
      "Decoded: 0000000000000000\n",
      "Stego:   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Step 12 | L_sem: 0.000 | L_rec: 0.693\n",
      "Secret:  1001101100010110\n",
      "Decoded: 0000000000000000\n",
      "Stego:   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "\n",
      "Step 13 | L_sem: 0.000 | L_rec: 0.685\n",
      "Secret:  0001101101100000\n",
      "Decoded: 0000000000000000\n",
      "Stego:   ry\n",
      "Ary\n",
      "Ary\n",
      "Ary\n",
      "Ary\n",
      "Ary\n",
      "\n",
      "Step 14 | L_sem: 0.000 | L_rec: 0.697\n",
      "Secret:  1000011111110010\n",
      "Decoded: 0000000000000000\n",
      "Stego:   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m optimizer.zero_grad()\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % \u001b[32m1\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     stego_text = \u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarrier_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecret_bits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     secret_str = bits_to_str(secret_bits.flatten())\n\u001b[32m     69\u001b[39m     decoded_secret_bits = decode(stego_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mencode\u001b[39m\u001b[34m(carrier, secret_bits)\u001b[39m\n\u001b[32m      3\u001b[39m prompt = build_generator_prompt(carrier, secret_bits)\n\u001b[32m      4\u001b[39m inputs = generator_tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m input_embeddings = generate_input_embeddings(\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, secret_bits)\n\u001b[32m      7\u001b[39m ids = generator.generate(\n\u001b[32m      8\u001b[39m     inputs_embeds=input_embeddings,\n\u001b[32m      9\u001b[39m     max_new_tokens=STEGO_LENGTH + SKIPPED_OUTPUT_TOKENS, \u001b[38;5;66;03m# First X tokens seem to be constant part of the LMs output\u001b[39;00m\n\u001b[32m     10\u001b[39m     eos_token_id=\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# prevent early stop\u001b[39;00m\n\u001b[32m     11\u001b[39m     do_sample=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m selected_ids = ids[\u001b[32m0\u001b[39m, SKIPPED_OUTPUT_TOKENS:]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    list(prefix_encoder.parameters()) +\n",
    "    list(generator.parameters()) + \n",
    "    list(decoder.parameters()),\n",
    "    lr=3e-5\n",
    ")\n",
    "\n",
    "for step in range(1000):\n",
    "    carrier_prompt = \"Explain how neural networks learn.\"\n",
    "    secret_bits = generate_secret().to(device)\n",
    "\n",
    "    prompt = build_generator_prompt(carrier_prompt, secret_bits)\n",
    "    generator_inputs = generator_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Prepare embeddings for first step by adding secret prefix to token embeddings\n",
    "    soft_embeds = generate_input_embeddings(generator_inputs, secret_bits)\n",
    "    past_kv = None\n",
    "\n",
    "    # Generate N tokens, store embeddings for each token\n",
    "    generated_embeds = []\n",
    "    for t in range(STEGO_LENGTH + SKIPPED_OUTPUT_TOKENS):   \n",
    "        stego_logits, token_probs, past_kv = generator_forward(\n",
    "            inputs_embeds=soft_embeds,\n",
    "            past_key_values=past_kv\n",
    "        )\n",
    "\n",
    "        soft_embeds = soft_embeddings(token_probs)  # [ B, D ]\n",
    "\n",
    "        # HACK: Modulate embeddings based on secret bit to test decoder\n",
    "        if (t >= SKIPPED_OUTPUT_TOKENS):\n",
    "            bit = secret_bits[0,t - SKIPPED_OUTPUT_TOKENS]\n",
    "            soft_embeds = torch.mul(soft_embeds, bit)\n",
    "\n",
    "        generated_embeds.append(soft_embeds)\n",
    "\n",
    "        # Take soft embeds for next step\n",
    "        soft_embeds = soft_embeds.unsqueeze(1)\n",
    "\n",
    "    #print(f'Number of generated embeddings: {len(generated_embeds)}, skipping first {SKIPPED_OUTPUT_TOKENS}...')\n",
    "\n",
    "    # Remove fixed first X tokens from output\n",
    "    generated_embeds = generated_embeds[SKIPPED_OUTPUT_TOKENS:]\n",
    "\n",
    "    # Stack embeddings for each token to [ B, N, D ]\n",
    "    encoder_outputs_embeds = torch.stack(generated_embeds, dim=1).to(device)\n",
    "\n",
    "    #print(f'Encoder outputs embeddings shape: {encoder_outputs_embeds.shape}')\n",
    "\n",
    "    # Calculate recovery loss based on decoding success\n",
    "    recovery_loss = decoder_forward(encoder_outputs_embeds, secret_bits.to(device))\n",
    "\n",
    "    # Calculate semantic loss compared to base model\n",
    "    #base_logits = anchor_forward(semantic_anchor, generator_inputs)\n",
    "    #semantic_loss = kl_semantic_loss(stego_logits, base_logits)\n",
    "    semantic_loss = np.zeros(1)\n",
    "\n",
    "    # Combine loss\n",
    "    loss = recovery_loss\n",
    "    #loss = 1.0 * recovery_loss + 0.0 * semantic_loss\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 1 == 0:\n",
    "        stego_text = encode(carrier_prompt, secret_bits)\n",
    "        secret_str = bits_to_str(secret_bits.flatten())\n",
    "        decoded_secret_bits = decode(stego_text)\n",
    "        decoded_secret_str = bits_to_str(decoded_secret_bits)\n",
    "\n",
    "        print(f\"Step {step} | L_sem: {semantic_loss.item():.3f} | L_rec: {recovery_loss.item():.3f}\")\n",
    "        print(f'Secret:  {secret_str}')\n",
    "        print(f'Decoded: {decoded_secret_str}')\n",
    "        print(f'Stego:   {stego_text}')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf93776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9042,   867,  5115,   359,   253,  7132,  1743,   338,   338,   338,\n",
      "           338,   338,   338,   338,  9218,   338,  1848,  1848,   338,   338,\n",
      "            30,  1069,   314,   314,   314,  1431, 17327,    30,   198,    49,\n",
      "          7132,  1743,    30,   198,  4093,   198,    49,   932,  3108,   282,\n",
      "           314,   253, 47126,   253,    30,   198,    49,   932,  3108,   282,\n",
      "           314,  1589,   314, 32195,    30,   198,    49,  3951,    30]])\n",
      "torch.Size([1, 59])\n"
     ]
    }
   ],
   "source": [
    "stego_text\n",
    "input_ids = generator_tokenizer(stego_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "print(input_ids)\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb69447",
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier = \"Write a short paragraph explaining how neural networks learn.\"\n",
    "secret = generate_secret().to(device)\n",
    "\n",
    "stego_text = encode(carrier, secret)\n",
    "recovered_secret = decode(stego_text)\n",
    "\n",
    "print(\"Stego text:\")\n",
    "print(stego_text)\n",
    "print()\n",
    "print(\"Original secret:\", bits_to_str(secret))\n",
    "print(\"Recovered secret:\", bits_to_str(recovered_secret))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stega",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
