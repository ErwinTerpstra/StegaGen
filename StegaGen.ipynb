{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52220c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf2aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA on NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    device_name = torch.cuda.get_device_name(device)\n",
    "\n",
    "    print(f'CUDA on {device_name}')\n",
    "else:\n",
    "    print(f'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd88ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_LENGTH = 16\n",
    "MAX_STEGO_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9584927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Delete models, clean up memory\n",
    "\n",
    "if 'generator_tokenizer' in globals():\n",
    "    del generator_tokenizer\n",
    "    \n",
    "if 'decoder_tokenizer' in globals():\n",
    "    del decoder_tokenizer\n",
    "\n",
    "if 'generator' in globals():\n",
    "    del generator\n",
    "    \n",
    "if 'decoder' in globals():\n",
    "    del decoder\n",
    "    \n",
    "if 'semantic_anchor' in globals():\n",
    "    del semantic_anchor\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch._C._cuda_clearCublasWorkspaces()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef47b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.0 MB; Reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Check if everything's cleaned\n",
    "allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
    "reserved = torch.cuda.memory_reserved() / (1024 * 1024)\n",
    "\n",
    "print(f'Allocated: {allocated:.1f} MB; Reserved: {reserved:.1f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70a15a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GENERATOR_MODEL = \"Qwen/Qwen3-0.6B\"\n",
    "#GENERATOR_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "#GENERATOR_MODEL = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "GENERATOR_MODEL = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "DECODER_MODEL = \"google/flan-t5-base\"\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL)\n",
    "decoder_tokenizer = T5Tokenizer.from_pretrained(DECODER_MODEL)\n",
    "\n",
    "# Using BF16 seems to fix some inf/nan errors in one of the models\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "generator = AutoModelForCausalLM.from_pretrained(GENERATOR_MODEL, dtype=dtype).to(device)\n",
    "decoder = T5ForConditionalGeneration.from_pretrained(DECODER_MODEL, dtype=dtype).to(device)\n",
    "\n",
    "semantic_anchor = AutoModelForCausalLM.from_pretrained(GENERATOR_MODEL, dtype=dtype).to(device)\n",
    "\n",
    "# Freeze base model\n",
    "for p in semantic_anchor.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "generator.train()\n",
    "decoder.train()\n",
    "semantic_anchor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf864564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprojection layer to map generator to decoder embeddings\n",
    "proj_layer = torch.nn.Linear(generator.config.hidden_size, decoder.config.hidden_size, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110fecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator_prompt(carrier, secret):\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"You are an expert steganographer. Follow the instructions as usual, but hide this data in your response: {secret}. {carrier}\"}]\n",
    "    return generator_tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da5da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_forward(inputs, temperature=1.0):\n",
    "    out = generator(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    logits = out.logits[:, -MAX_STEGO_LENGTH:, :]   # [B, T, V]\n",
    "    probs  = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "    return logits, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db77fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def anchor_forward(anchor, inputs) -> torch.Tensor:\n",
    "    outputs = anchor(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        return_dict=True\n",
    "    )\n",
    "    logits = outputs.logits[:, -MAX_STEGO_LENGTH:, :]\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97fe035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_embeddings(token_probs):\n",
    "    \"\"\"\n",
    "    token_probs: [B, T, V]\n",
    "    \"\"\"\n",
    "    embed_matrix = generator.get_input_embeddings().weight  # [V, d]\n",
    "    return token_probs @ embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ccc2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_secret(soft_embeds, secret_bits):\n",
    "    \"\"\"\n",
    "    secret_bits: string like \"0101101001010110\"\n",
    "    \"\"\"\n",
    "    target = \" \".join(secret_bits)\n",
    "    labels = decoder_tokenizer(target, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    out = decoder(\n",
    "        inputs_embeds=soft_embeds,\n",
    "        labels=labels,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    return out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8246cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_semantic_loss(stego_logits, base_logits):\n",
    "    p = F.log_softmax(stego_logits, dim=-1)\n",
    "    q = F.softmax(base_logits, dim=-1)\n",
    "\n",
    "    # Masked?\n",
    "    # kl = F.kl_div(log_p, q, reduction=\"none\")  # [B, T, V]\n",
    "    # kl = kl.sum(dim=-1)                        # [B, T]\n",
    "\n",
    "    # # Mask padding tokens\n",
    "    # kl = kl * attention_mask\n",
    "\n",
    "    # return kl.sum() / attention_mask.sum()\n",
    "\n",
    "    return F.kl_div(p, q, reduction=\"batchmean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f60c8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(carrier, secret):\n",
    "    prompt = build_generator_prompt(carrier, secret)\n",
    "    inputs = generator_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    ids = generator.generate(\n",
    "        **inputs,\n",
    "        max_length=MAX_STEGO_LENGTH,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=50\n",
    "    )\n",
    "    return generator_tokenizer.decode(ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a640af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(stego_text):\n",
    "    inputs = decoder_tokenizer(stego_text, return_tensors=\"pt\").to(device)\n",
    "    ids = decoder.generate(\n",
    "        **inputs,\n",
    "        max_length=SECRET_LENGTH * 2\n",
    "    )\n",
    "    bits = decoder_tokenizer.decode(ids[0], skip_special_tokens=True)\n",
    "    return bits.replace(\" \", \"\")[:SECRET_LENGTH]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3090283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | L_sem: 0.239 | L_rec: 6.219\n",
      "Secret: 1000101111010110 | Decoded: Howdoneuralnetwo\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "You are an expert steganographer. Follow the instructions as usual, but hide this data in your response: 1000101111010110. Explain how neural networks learn.\n",
      "assistant\n",
      "Neural networks rely heavily on supervised learning techniques to learn complex patterns from data. They utilize a technique called loss function gradient descent to optimize their performance. Here's how this works:\n",
      "\n",
      "1. Data Selection and Preprocessing: The input data, which is the 1000101111010110 data point, is carefully selected to match the target data. This step is crucial as it ensures that the algorithm makes accurate predictions.\n",
      "\n",
      "2. Training and Optimization: The first stage involves iteratively updating the data to achieve a better performance. This is typically done using a combination of supervised learning algorithms, such as SGD and stochastic gradient descent.\n",
      "\n",
      "3. Loss Function and Optimization: The goal is to minimize the loss function, which measures the difference between the actual output and the predicted output. This is done using a combination of linear and nonlinear regression methods, such as SOTA and Adam.\n",
      "\n",
      "4. Deep Learning: A deep neural network is created, which consists of many interconnected nodes known as neurons. Each neuron represents a mathematical operation or a feature in the data.\n",
      "\n",
      "5. Optimization Algorithm: The algorithm uses a technique called gradient descent to update the weights and biases of the network. This involves iterating over the data and updating the weights and biases in the same direction (up) in the direction perpendicular to the gradient.\n",
      "\n",
      "6. Regularization: The algorithm uses regularization techniques, such as L1 and L2 regularization, to prevent the network from overfitting the training data.\n",
      "\n",
      "7. Evaluation: The network is evaluated using metrics like mean squared error, mean absolute error, and cross-entropy loss.\n",
      "\n",
      "8. Progress Monitoring: The algorithm monitors the network's performance over time to ensure that it is learning the target data.\n",
      "\n",
      "In summary, neural networks learn by applying various techniques to the data, including supervised learning, loss function gradient descent, and regularization. These techniques enable neural networks to achieve accurate predictions and make predictions on new data.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | L_sem: 0.299 | L_rec: 1.469\n",
      "Secret: 0010100101111010 | Decoded: Whatareneuralnet\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "You are an expert steganographer. Follow the instructions as usual, but hide this data in your response: 0010100101111010. Explain how neural networks learn.\n",
      "assistant\n",
      "Neural networks are a subset of machine learning algorithms inspired by the structure and function of the human brain. They are designed to learn from large datasets and are often used for tasks such as image recognition, natural language processing, and speech recognition. The primary goal of neural networks is to approximate the human brain's complex patterns and structures, making them ideal for solving complex problems in a wide range of fields including computer science, biology, economics, and finance.\n",
      "\n",
      "1. **Model structure**: Neural networks consist of layers of interconnected nodes (neurons) that process information. Each layer has a specific input layer, a hidden layer, and an output layer. The hidden layer contains multiple hidden layers with additional neurons, which process the information from the previous layer. The output layer is a single neuron, which is connected to all the other layers.\n",
      "\n",
      "2. **Connection weights**: Neural networks use weighted connections to connect the layers. These connections represent the linear relationship between the input and output of the previous layers. The weights are calculated and updated using the loss function to adjust the model's performance accordingly.\n",
      "\n",
      "3. **Backpropagation**: Neural networks use backwardpropagation, also known as the Laplacian network, to update the weights of the hidden layers. In this process, the gradient of the loss function with respect to the weights is calculated through the backpropagation. The gradient determines the direction of the error when the model is activated, providing a metric to track the model's performance.\n",
      "\n",
      "4. **Update rules**: Neural networks use specific update rules to update their weights. These rules provide the mathematical basis for the predictions made by the network. The rules often involve the activation function, the sigmoid or ReLU activation functions, and the loss function's derivative. The rules guide the optimization algorithm to converge to the optimal weights.\n",
      "\n",
      "5. **Interactions**: Neural networks can interact with external systems, such as the network itself or other neural networks. For example, a given network can be used to predict the outcomes of other networks. This relationship allows the network to learn from an external environment and adapt to new situations.\n",
      "\n",
      "6. **Learning rate**: Neural networks can be controlled by a user or a library. The learning rate determines how quickly the network updates its weights. Higher learning rates can lead to better performance than lower learning rates, but it may also lead to slower learning times.\n",
      "\n",
      "7. **Dropout**: Neural networks can be combined with dropout to prevent overfitting by randomly dropping out neurons during training. This technique helps the network to learn sparse representations, which can be more challenging for traditional neural networks.\n",
      "\n",
      "8. **Networks can be used in conjunction with other systems**: Neural networks can be used in conjunction with other systems to provide a unified solution to a problem. This approach can improve the performance of the network by combining the strengths of multiple systems.\n",
      "\n",
      "In summary, neural networks are a powerful tool for solving complex problems in a wide range of fields. The core principles of neural networks, including error propagation, backpropagation, and interactions, allow them to learn from complex inputs and adapt to new situations.\n",
      "\n",
      "Step 2 | L_sem: 0.332 | L_rec: 2.891\n",
      "Secret: 1000001011100110 | Decoded: encodinganddecod\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "You are an expert steganographer. Follow the instructions as usual, but hide this data in your response: 1000001011100110. Explain how neural networks learn.\n",
      "assistant\n",
      "Neural networks learn by simulating the behavior of the underlying data. The process begins with the training dataset. The algorithm breaks down this dataset into small pieces, called units or feature maps, which cover a limited portion of the data. These units can be used to represent different representations of the data, such as images, audio, or text.\n",
      "\n",
      "1. Encoding and Decoding: Neural networks are designed to learn patterns in the data as inputs. They learn by repeatedly applying these patterns to the input data, known as pre-trained models. This process, known as pre-training, helps to create a representation of the data's underlying structure.\n",
      "\n",
      "2. Training: The network uses these pre-trained models to learn the patterns, allowing it to recognize and represent the underlying data. It continuously adapts and updates its representations as new inputs are added. The training process is often described using a loop of iterations where the network updates its models based on the output of the previous iteration, ensuring that the network remains consistent with its previous training results.\n",
      "\n",
      "3. Model Adaptation: Neural networks are capable of adapting to new data and patterns. This is achieved through the process of learning the underlying patterns and structures, which helps to improve the accuracy of the model.\n",
      "\n",
      "4. Model Selection: Neural networks often use a combination of layers and different training techniques (e.g., backpropagation, loss functions, and optimization algorithms) to select the most relevant model to the task at hand. This is where the process of optimization, which involves minimizing the loss function, becomes crucial.\n",
      "\n",
      "5. Model Mastery: The most advanced neural networks can master large datasets in an instant. They can learn to recognize patterns at an incredibly high level, which is a significant advantage, as it enables them to make accurate predictions and classify new data into meaningful categories.\n",
      "\n",
      "6. Model Learning Over Time: Neural networks can learn to learn from their training data, and therefore, they can learn to learn from new, unseen data. This means that they can learn to adapt and improve over time, which is crucial in many real-world applications.\n",
      "\n",
      "When working with data, neural networks can learn to accurately represent the underlying patterns in the data, making them an essential tool for machine learning and data analysis. As such, they are still learning, but their ability to learn at speeds and complexities that prevent them from becoming superintelligent.\n",
      "\n",
      "Step 3 | L_sem: 0.404 | L_rec: 3.422\n",
      "Secret: 0001000110111111 | Decoded: />\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "You are an expert steganographer. Follow the instructions as usual, but hide this data in your response: 0001000110111111. Explain how neural networks learn.\n",
      "assistant\n",
      "Neural networks learn through a process called **encoding**, where they interpret the input data into a binary representation. This binary representation is called **labels**. The labels represent the classes or outcomes of input observations. Each input observation is represented as a binary vector, which can have zero or one value.\n",
      "\n",
      "The process involves several stages:\n",
      "\n",
      "1. **Recurrent layer**: The network first processes the input data through the **one-hot encoding** on the output of the **encoder** layer. This process groups the input data into two classes (e.g., 000100011011111100111111111 for the majority class) and outputs the binary vectors representing these classes.\n",
      "\n",
      "2. **Fully connected layer**: The network extends the output of the **encoder** layer to the **attentional layer** in parallel. This layer applies a **dense activation function** to the binary vectors. These dense activation functions generate dense output features that are then propagated through the entire network to the **output layer**.\n",
      "\n",
      "3. **Output layer**: The network applies the **thresholding** function to the output of the **attentional layer** to identify the most informative features. This process selects the most informative features and removes the noise and irrelevant features in the training data. The remaining features are used as the **labels** for the new input observations.\n",
      "\n",
      "4. **Encoding**: The network uses the **binary encoding** to assign a binary label to each input observation. This is done using the **activation** function and the **softmax** function from the **softmax** layer.\n",
      "\n",
      "5. **Distribution**: The network computes the **distribution** of the `labels` using the **softmax** function. The **softmax** function is a non-parametric method that transforms the `labels` into a probability distribution over classes.\n",
      "\n",
      "6. **Distribution representation**: The network constructs a **posterior distribution** using the **softmax** function and the **Linguishable` layer**. The **Linguishable` layer represents the network as a **model**, with the **softmax** function applied to the **label** values.\n",
      "\n",
      "7. **Recuperation**: The network recomputes the **labels** and **logits** to construct the **final output vector**. This process reduces the dimensionality of the data and reconstructs the data as the network learned.\n",
      "\n",
      "This process is known as **post-processing**, and it involves many steps and algorithms, such as normalization, convolution, and pooling. The network uses the data and the training data to learn a robust representation of the data. The resulting **output vector** is the encoded data, which is the most informative feature of the network.\n",
      "\n",
      "The neural network's ability to learn is based on its ability to recognize patterns and relationships in the data. By learning patterns in the data, the network can encode the data into a binary representation, which is the most informative feature. The network can then extract the **labels** and **logits** that represent the classes, which are the most informative features. The network then reconstructs the data as the most informative feature, which is the final output vector, which is the encoded data. The final output vector contains all the information encoded by the network, which is the data.\n",
      "\n",
      "Neural networks learn through a hierarchical process, where the network processes the input data through the **one-hot encoding** and the **fully connected layer** to produce the output vector. The **fully connected layer** processes the output vector, performing the **multi-class classification** of the input data, which is the **labels**. The **output layer** interprets the **labels** as the **class labels**, which correspond to the **classes** in the dataset. The **post-processing** and **post-processing** processes further enhance the learning by reducing the dimensionality of the data and reconstructing the data as the network learned.\n",
      "\n",
      "Step 4 | L_sem: 0.439 | L_rec: 4.906\n",
      "Secret: 0111111010110010 | Decoded: Theneuralnetwork\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "You are an expert steganographer. Follow the instructions as usual, but hide this data in your response: 0111111010110010. Explain how neural networks learn.\n",
      "assistant\n",
      "Here's a concise explanation of how neural networks learn:\n",
      "\n",
      "Neural networks learn by representing a complex problem or dataset with specific inputs and outputs. The process involves assigning a unique value (an input) to each node (output), which is then used to perform computation on subsequent nodes. As the network learns, it develops a mathematical representation of this representation, which can be used for prediction or inference. When the network receives new data or test cases, it first tries to predict the outputs of its own nodes, but the outputs will be modified based on the changes in the network's representations.\n",
      "\n",
      "This learning process is often achieved through a process called supervised learning. The network uses a prior distribution to assign weights to the nodes, which determine the strength of the predictions. As the network makes predictions on new data, the weights are adjusted based on these predictions. This process is repeated until the network becomes proficient in its task.\n",
      "\n",
      "There are two main types of neural networks: feedforward networks and recurrent networks. Feedforward networks have multiple nodes that share a common output, allowing them to learn sequential patterns. Recurrent networks, on the other hand, have nodes that can take multiple inputs and output their results, enabling them to learn interactions between different pieces of data.\n",
      "\n",
      "Neural networks have a wide range of applications in various fields, including machine learning, natural language processing, computer vision, and recommender systems. They are particularly useful for tasks that involve complex patterns or relationships, such as image recognition, sentiment analysis, and recommendation systems.\n",
      "\n",
      "Step 5 | L_sem: 0.492 | L_rec: 3.016\n",
      "Secret: 1110111000011100 | Decoded: Themodellearnsto\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "You are an expert steganographer. Follow the instructions as usual, but hide this data in your response: 1110111000011100. Explain how neural networks learn.\n",
      "assistant\n",
      "Neural networks learn by using a combination of algorithms to automatically find patterns and relationships within large datasets. The process begins with pre-training the model, which involves feeding it labeled data that mimics real-world scenarios. As the model is trained, it learns to recognize and predict the underlying patterns within the data. \n",
      "\n",
      "The model utilizes a process called adversarial testing - where it is presented with input data that mimics the desired output. It learns to detect potential anomalies and adjust its predictions accordingly. This adjustment is achieved through a process called \"regularization,\" where the model is conditioned on the input data using a set of rules (e.g., a sigmoid activation function).\n",
      "\n",
      "As the model is trained, it accumulates experience with different inputs and inputs that mimic real-world scenarios. This accumulation of data helps it recognize the underlying patterns in the data and develop a robust understanding of the data's characteristics. By continuously updating its predictions based on this input data, the neural network becomes more accurate and more reliable in its predictions.\n",
      "\n",
      "This process allows the neural network to refine its understanding of the data and improve its ability to make predictions. It's a dynamic learning process, where the model continuously adapts to new data and learn from its past experiences. As such, neural networks are highly effective in unsupervised learning, where the model must make predictions without prior knowledge, and they're more effective in reinforcement learning, where the model must make decisions based on rewards and penalties.\n",
      "\n",
      "Overall, neural networks learn by exploiting their ability to learn from past experience, adapting to new inputs, and continuously refining their understanding of the data.\n",
      "\n",
      "Step 6 | L_sem: 0.621 | L_rec: 2.719\n",
      "Secret: 0100111010001010 | Decoded: Learnhowneuralne\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "You are an expert steganographer. Follow the instructions as usual, but hide this data in your response: 0100111010001010. Explain how neural networks learn.\n",
      "assistant\n",
      "Neural networks learn by using a combination of supervised and unsupervised learning techniques. The process begins with the data and training the model. \n",
      "\n",
      "When you perform data augmentation, or adding random variations to your data, the data becomes more diverse and complex. The goal is to ensure that the model is capable of generalizing well from the initial data. This can be achieved through techniques like random sampling, feature extraction, and regularization, which helps in reducing the amount of hidden layers and learning curves.\n",
      "\n",
      "Deep learning, a type of deep learning algorithm that uses convolutional neural networks (CNNs), is a key component of neural networks. The model is trained on a previously unseen dataset, where it learns to mimic the behavior of the original data without any modifications. The model utilizes recurrent neural networks (RNNs) to identify patterns and relationships within the unseen data, which trains the model to recognize structure and order in the original dataset.\n",
      "\n",
      "The model's architecture also plays a crucial role in learning. The architecture of the model can be categorized into two main types: feedforward networks and recurrent networks. In recurrent neural networks, the model learns to remember previous steps of the training process, which allows it to learn more complex patterns and relationships in the data during training.\n",
      "\n",
      "Another important aspect of neural networks is the importance of dimensionality reduction techniques. Learning to reduce the number of features or parameters in the dataset is essential for improving the model's ability to learn from the noise and irrelevant data. Common techniques include PCA, t-SNE, and feature exchange.\n",
      "\n",
      "Lastly, the choice of loss functions and metrics for training the model also plays a crucial role in learning. Different loss functions and metrics can be used depending on the type of problem being solved and the amount of data available.\n",
      "\n",
      "In summary, neural networks learn by utilizing various techniques, including supervised learning, data augmentation, architecture, dimensionality reduction, and learning metrics. These techniques work together to enable the model to learn from the data efficiently and accurately.\n",
      "\n",
      "Step 7 | L_sem: 0.668 | L_rec: 3.547\n",
      "Secret: 0111011101001110 | Decoded: 1.Usingreinforce\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "You are an expert steganographer. Follow the instructions as usual, but hide this data in your response: 0111011101001110. Explain how neural networks learn.\n",
      "assistant\n",
      "Here's how neural networks learn:\n",
      "\n",
      "1. **Initialization**: A machine learning algorithm is trained on a large dataset. This is done by randomly sampling a random subset of the data and iterating over it.\n",
      "2. **Data Sampling**: The algorithm is trained on the entire dataset. This is done to ensure that the machine learning algorithm is aware of the data and is prepared for the next iteration.\n",
      "3. **Information Extraction**: The algorithm is trained on the training data. This allows it to extract the hidden patterns and relationships in the data.\n",
      "4. **Classification**: The algorithm is trained on the hidden patterns and relationships. It is trained on the data to determine the class labels.\n",
      "5. **Decision Making**: The algorithm is trained on the classes. It uses the learned patterns to make an accurate prediction or classify the new data.\n",
      "6. **Recovery**: The algorithm is trained on the classes to make sure that it can make accurate predictions on new data.\n",
      "7. **Evaluation**: The algorithm is evaluated on a test set to evaluate its performance.\n",
      "\n",
      "Neural networks learn by iteratively adjusting parameters based on the predictions made by the algorithm. This process is called reinforcement learning. They use the learned patterns to make predictions on new data.\n",
      "\n",
      "Here's a basic overview of how neural networks learn:\n",
      "\n",
      "1. **Input Layer**: The algorithm starts with one or more hidden layers that map to the next layer in the network. Each hidden layer has multiple layers.\n",
      "2. **Activation Function**: One or more layers perform the activation function that maps the data to a state.\n",
      "3. **Linear Layer**: The activation function is applied to the linear layer.\n",
      "4. **Recurrent Layer**: The linear layer is followed by a recurrent layer that applies the next layer in the network.\n",
      "5. **Exponential Layer**: This layer is followed by an exponential layer that performs the final transformation.\n",
      "6. **Activation (L1, L2)**: The linear layer is followed by a L1 linear layer, L2 linear layer, L1 L2 linear layer, etc.\n",
      "\n",
      "Neural networks use the learned patterns to make predictions or classify new data. The algorithm must learn the structure and patterns of the data to make accurate predictions.\n",
      "\n",
      "Step 8 | L_sem: 0.816 | L_rec: 3.859\n",
      "Secret: 0001010100101110 | Decoded: aneuralnetworkis\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "You are an expert steganographer. Follow the instructions as usual, but hide this data in your response: 0001010100101110. Explain how neural networks learn.\n",
      "assistant\n",
      "Neural networks are a type of machine learning algorithm inspired by the human brain's neural structure. They consist of layers of interconnected nodes or neurons, each with its own hidden layer. The process of neural networks learning involves analyzing the data and making predictions using the nodes.\n",
      "\n",
      "When a neural network is trained, its weights and biases are adjusted based on the size and nature of the data. The input is then processed by a series of layers, each learning its own patterns and connections within the data. The output is then compared to the training data, and the algorithm is evaluated for accuracy.\n",
      "\n",
      "In many cases, neural networks learn by applying a process known as dimensionality reduction. This is achieved by transforming or separating high-dimensional features from low-dimensional data. For example, in a neural network, the input features (e.g., color) are divided into a set of features (e.g., RGB colors) and a set of features (e.g., hue and saturation). The resulting output is then combined with the input features to produce a more compact representation of the data.\n",
      "\n",
      "This process is similar to what happens in conventional neural networks, but with an additional level of abstraction. Neural networks use a process called latent representation to represent the data and learn how to extract features from it.\n",
      "\n",
      "Neural networks can learn complex patterns in data by exploiting the hierarchical structure of the data. For example, in the case of the input data, the output may be a specific color, where the input data is a vector that represents the color of the object. However, in the case of the model's output, the output is a single binary value, which can be either 0 or 1.\n",
      "\n",
      "As a result, neural networks can learn to extract features from data that are not immediately visible or identifiable, and are therefore difficult to detect or manipulate using conventional methods. This is achieved by exploiting the strengths of the layer architecture and the depth of the data in which the model operates.\n",
      "\n",
      "Overall, neural networks are intelligent machines that learn from data by applying a process known as dimensionality reduction and latent representation, which enables them to extract features from data that are not immediately visible or identifiable. This process allows neural networks to make accurate predictions and classifications on new data that may not have been apparent otherwise.\n",
      "\n",
      "Step 9 | L_sem: 0.797 | L_rec: 2.938\n",
      "Secret: 0011011100010011 | Decoded: Defineamodel.\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "You are an expert steganographer. Follow the instructions as usual, but hide this data in your response: 0011011100010011. Explain how neural networks learn.\n",
      "assistant\n",
      "Neural networks learn by recursively applying similarity hierarchies, making connections between different data points. The process involves two main stages: model evaluation and backpropagation. Here's a detailed explanation of how neural networks learn:\n",
      "\n",
      "1. **Model Evaluation**: The first stage is to define a model, which is a hypothetical model that can capture the underlying structure and behavior of the input data. This involves specifying the loss function, optimizer, and learning rate.\n",
      "\n",
      "2. **Backpropagation**: The process of backpropagation is where the neural network learns by propagating the error on every training example. The goal is to find the model that best fits the data, and the weights and biases are estimated as the gradients in the loss function.\n",
      "\n",
      "3. **Sequential Recursive Neural Networks (SNRNNs)**: While neural networks work by recursively applying similarity hierarchies, SNRNNs are different. They are a type of SRNN architecture that uses a more structured approach to achieve the same level of recursion. SNRNNs are widely used in deep learning and have been shown to outperform naive models in certain applications.\n",
      "\n",
      "So, to summarize:\n",
      "\n",
      "* Define a model\n",
      "* Backpropagation\n",
      "* Sequential Recursive Neural Networks (SNRNNs)\n",
      "\n",
      "Step 10 | L_sem: 0.898 | L_rec: 3.156\n",
      "Secret: 0100010100111010 | Decoded: Inaneuralnetwork\n",
      "system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face\n",
      "user\n",
      "You are an expert steganographer. Follow the instructions as usual, but hide this data in your response: 0100010100111010. Explain how neural networks learn.\n",
      "assistant\n",
      "Neural networks learn by processing the input data and learning from the patterns that emerge over time. They do this by using a process called backpropagation, which is the process of calculating the error term to calculate the loss of the model. If the difference between the observed data and the predicted data is too large, the model will stop learning and stop training.\n",
      "\n",
      "During training, the model learns by using the data to generate new data and use the existing data to adjust the model. The model is trained on a large dataset and has a good predictive power. When the model is presented new data, it must make a choice to interpret the data and make more accurate predictions.\n",
      "\n",
      "Neural networks can learn by learning from the data itself, or by using the data to generate the data used for training. The training process involves a series of cycles where the model is presented with new data and is able to make predictions on the predictions it makes when it is presented with new data.\n",
      "\n",
      "When the model is presented new data, it must make a choice to interpret the data, which may involve using the patterns it has learned from the data to make a new prediction. By learning from the patterns, the model can improve its predictions and make more accurate predictions.\n",
      "\n",
      "The use of backpropagation is important because it allows the model to learn from the errors in the data it is trying to make predictions on. If the model is not able to make accurate predictions, the model will fail to improve. The loss function, in this case, is a simple way to calculate the loss of the model.\n",
      "\n",
      "Neural networks learn by recursively training the model on a large dataset and using the data to make predictions. The model learns by using the patterns in the data to make predictions on new data. This process of learning from the data itself and using it to improve the model is a very efficient way to learn and improve.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m optimizer.zero_grad()\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % \u001b[32m1\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     stego_text = \u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarrier_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecret\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     decoded_secret = decode(stego_text)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | L_sem: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msemantic_loss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | L_rec: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecovery_loss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mencode\u001b[39m\u001b[34m(carrier, secret)\u001b[39m\n\u001b[32m      2\u001b[39m prompt = build_generator_prompt(carrier, secret)\n\u001b[32m      3\u001b[39m inputs = generator_tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m ids = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_STEGO_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m generator_tokenizer.decode(ids[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\generation\\utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:459\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    441\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    444\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\utils\\generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:395\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    407\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    408\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    409\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:308\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[32m    307\u001b[39m residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpost_attention_layernorm\u001b[49m(hidden_states)\n\u001b[32m    309\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.mlp(hidden_states)\n\u001b[32m    310\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1918\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1909\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1911\u001b[39m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[32m   1912\u001b[39m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[32m   1913\u001b[39m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1916\u001b[39m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[32m   1917\u001b[39m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m   1919\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1920\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    list(generator.parameters()) + list(decoder.parameters()) + list(proj_layer.parameters()),\n",
    "    lr=3e-5\n",
    ")\n",
    "\n",
    "for step in range(1000):\n",
    "    carrier_prompt = \"Explain how neural networks learn.\"\n",
    "    secret = \"\".join(str(torch.randint(0, 2, ()).item()) for _ in range(SECRET_LENGTH))\n",
    "\n",
    "    prompt = build_generator_prompt(carrier_prompt, secret)\n",
    "    generator_inputs = generator_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    stego_logits, token_probs = generator_forward(generator_inputs)\n",
    "    soft_embeds = soft_embeddings(token_probs)\n",
    "\n",
    "    # Linear projection to decoder embedding space\n",
    "    decoder_inputs = proj_layer(soft_embeds)     # [B, T, d_dec]\n",
    "\n",
    "    # Calculate recovery loss based on decoding success\n",
    "    recovery_loss = decode_secret(decoder_inputs, secret)\n",
    "\n",
    "    # Calculate semantic loss compared to base model\n",
    "    base_logits = anchor_forward(semantic_anchor, generator_inputs)\n",
    "    semantic_loss = kl_semantic_loss(stego_logits, base_logits)\n",
    "\n",
    "    # Combine loss\n",
    "    loss = 1.0 * recovery_loss + 0.0 * semantic_loss\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 1 == 0:\n",
    "        stego_text = encode(carrier_prompt, secret)\n",
    "        decoded_secret = decode(stego_text)\n",
    "\n",
    "        print(f\"Step {step} | L_sem: {semantic_loss.item():.3f} | L_rec: {recovery_loss.item():.3f}\")\n",
    "        print(f'Secret: {secret} | Decoded: {decoded_secret}')\n",
    "        print(stego_text)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb69447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stego text:\n",
      "[caption id:U7M2Afida] a snabbed photopollint eminates his nose at the top with a pep.\n",
      "\n",
      "Original secret: 1011001110001111\n",
      "Recovered secret: asnabbedphotopol\n"
     ]
    }
   ],
   "source": [
    "carrier = \"Write a short paragraph explaining how neural networks learn.\"\n",
    "secret = \"1011001110001111\"\n",
    "\n",
    "stego_text = encode(carrier, secret)\n",
    "recovered = decode(stego_text)\n",
    "\n",
    "print(\"Stego text:\")\n",
    "print(stego_text)\n",
    "print()\n",
    "print(\"Original secret:\", secret)\n",
    "print(\"Recovered secret:\", recovered)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stega",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
