{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52220c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "import torch\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf2aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA on NVIDIA RTX 500 Ada Generation Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Determine what device to use\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    device_name = torch.cuda.get_device_name(device)\n",
    "\n",
    "    print(f'CUDA on {device_name}')\n",
    "else:\n",
    "    print(f'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd88ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "\n",
    "SECRET_LENGTH = 16\n",
    "TOKENS_PER_BIT = 1\n",
    "INJECT_SECRET_IN_SYSTEM_PROMPT = False \t# Whether to insert the secret as binary text in the system prompt\n",
    "MASK_TOKENS = False\t\t\t\t\t\t# Whether to mask a subset of tokens basd on the encoded secret bit. A simple form of encoding\n",
    "\n",
    "STEGO_LENGTH = SECRET_LENGTH * TOKENS_PER_BIT\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "MAX_EPOCH = 200                         # Maximum number of training epochs\n",
    "\n",
    "LR_INITIAL = 0.1\t\t\t\t\t\t# Initial learning rate\n",
    "LR_GAMMA = 0.98\t    \t\t\t\t\t# Learning rate decay factor per epoch\n",
    "\n",
    "RECOVERY_LOSS_ALPHA_START = 0.95\t\t# Weighting factor for recovery loss vs semantic loss at the start of training (0.0 to 1.0)\n",
    "RECOVERY_LOSS_ALPHA_END = 0.6\t\t\t# Weighting factor for recovery loss vs semantic loss at the end of training (0.0 to 1.0)\n",
    "RECOVERY_LOSS_STEPS = MAX_EPOCH // 2\t# Number of steps over which to linearly anneal the recovery loss weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf87eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt templates\n",
    "\n",
    "INPUT_PROMPT = 'Tell me a fact about {0}.'\n",
    "INPUT_SUBJECTS = [\n",
    "    'traveling',\n",
    "    'sports',\n",
    "    'music',\n",
    "    'technology',\n",
    "    'health and wellness',\n",
    "    'education',\n",
    "    'history',\n",
    "    'science',\n",
    "    'art and culture'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9584927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup previous run\n",
    "# This allows rerunning the entire notebook without memory leaks\n",
    "\n",
    "import gc\n",
    "\n",
    "# Delete models, clean up memory\n",
    "\n",
    "if 'generator_tokenizer' in globals():\n",
    "    del generator_tokenizer # type: ignore\n",
    "    \n",
    "if 'generator' in globals():\n",
    "    del generator # type: ignore\n",
    "    \n",
    "if 'decoder' in globals():\n",
    "    del decoder # type: ignore\n",
    "    \n",
    "if 'semantic_anchor' in globals():\n",
    "    del semantic_anchor # type: ignore\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch._C._cuda_clearCublasWorkspaces()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef47b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.0 MB; Reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Check if everything's cleaned\n",
    "allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
    "reserved = torch.cuda.memory_reserved() / (1024 * 1024)\n",
    "\n",
    "print(f'Allocated: {allocated:.1f} MB; Reserved: {reserved:.1f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c988c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stego Encoder Model Definition\n",
    "\n",
    "class StegoEncoder(nn.Module):\n",
    "    def __init__(self, base_lm, prefix_encoder = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_lm = base_lm\n",
    "        self.prefix_encoder = prefix_encoder\n",
    "        self.vocab_size = base_lm.config.vocab_size\n",
    "        \n",
    "        # Secret encoder to map secret bit to vocab size\n",
    "        self.logit_bias = nn.Parameter(torch.rand(self.vocab_size))\n",
    "\n",
    "        # Freeze base model\n",
    "        for p in self.base_lm.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def generate_input_embeddings(self, input_ids, secret_bits = None):\n",
    "        # Token embeddings based on input tokens\n",
    "        token_embeds = self.base_lm.get_input_embeddings()(input_ids) # (B, T, D)\n",
    "        if self.prefix_encoder is None or secret_bits is None:\n",
    "            return token_embeds\n",
    "        \n",
    "        # Prefix embeddings based on secret bits\n",
    "        prefix_embeds = self.prefix_encoder(secret_bits)\n",
    "\n",
    "        # Concatenate prefix and token embeddings\n",
    "        input_embeds = torch.cat([prefix_embeds, token_embeds], dim=1)  # (B, T+P, D)\n",
    "        \n",
    "        return input_embeds\n",
    "\n",
    "    def mask_indices(self, logits, bit):\n",
    "        # Create a mask to mask out subsets of tokens\n",
    "        logit_mask = torch.full((self.vocab_size, ), True)\n",
    "        \n",
    "        # bit = 0: mask even tokens\n",
    "        # bit = 1: mask odd tokens\n",
    "        masked_indices = list(range(bit.item(), self.vocab_size, 2))\n",
    "        logit_mask[masked_indices] = False\n",
    "\n",
    "        # Mask out tokens we don't allow\n",
    "        logits[:, ~logit_mask] = -math.inf\n",
    "\n",
    "    def forward(self, secret_bit = None, temperature=1.0, **kwargs):\n",
    "        out = self.base_lm(\n",
    "            **kwargs,\n",
    "            return_dict=True,\n",
    "            use_cache=True,\n",
    "            eos_token_id=None, # prevent early stop\n",
    "        )\n",
    "\n",
    "        # Select logits for the last token only (this is the next prediction)\n",
    "        logits = out.logits[:, -1, :]   # [B, V]\n",
    "\n",
    "        # Encode secret bit based on learned logit bias\n",
    "        if secret_bit is not None:\n",
    "            batch_bits = secret_bit.unsqueeze(1)  # [B, 1]\n",
    "\n",
    "            bit = (batch_bits * 2 - 1).expand(-1, self.vocab_size)  # [B, V]\n",
    "            bias = self.logit_bias.expand(bit.shape[0], -1) * bit   # [B, V]\n",
    "            \n",
    "            biased_logits = logits + bias\n",
    "\n",
    "            # Apply masking if enabled\n",
    "            if MASK_TOKENS:\n",
    "                self.mask_indices(biased_logits, secret_bit)\n",
    "        else:\n",
    "            biased_logits = logits\n",
    "\n",
    "        # Softmax to convert from logits to probabilities\n",
    "        probs = torch.softmax(biased_logits / temperature, dim=-1)\n",
    "\n",
    "        return biased_logits, logits, probs, out.past_key_values\n",
    "\n",
    "    def soft_embeddings(self, token_probs):\n",
    "        embed_matrix = self.base_lm.get_input_embeddings().weight  # [V, d]\n",
    "        return token_probs @ embed_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbad1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stego Decoder Model Definition\n",
    "\n",
    "class StegoDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, output_size=16, pool_size=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "        # Token-wise binary classification head\n",
    "        # Note: Sigmoid() seems to strongly decrease decoder effectiveness?\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(vocab_size * pool_size, 1),\n",
    "            #nn.GELU(),\n",
    "            #nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, token_probs=None, labels=None):\n",
    "        # Perform classification to get logits for each bit\n",
    "        logits = self.classifier(token_probs)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(logits.squeeze(-1), labels.float())\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a15a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator embedding dimensions: 576, vocabulary size: 49152\n"
     ]
    }
   ],
   "source": [
    "# Create LM models\n",
    "\n",
    "#GENERATOR_MODEL = \"Qwen/Qwen3-0.6B\"\n",
    "#GENERATOR_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "#GENERATOR_MODEL = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "GENERATOR_MODEL = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL)\n",
    "generator = AutoModelForCausalLM.from_pretrained(GENERATOR_MODEL).to(device)\n",
    "\n",
    "# Print model info\n",
    "embedding_size = generator.config.hidden_size\n",
    "vocab_size = generator.config.vocab_size\n",
    "print(f'Generator embedding dimensions: {embedding_size}, vocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95bdc5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StegoDecoder(\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=49152, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create decoder & decoder model\n",
    "\n",
    "encoder = StegoEncoder(base_lm=generator).to(device)\n",
    "encoder.train()\n",
    "\n",
    "decoder = StegoDecoder(vocab_size=vocab_size, embedding_size=embedding_size, output_size=SECRET_LENGTH, pool_size=TOKENS_PER_BIT).to(device)\n",
    "decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db77fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def generate_secret(batch_size=1):\n",
    "    \"\"\"Generate a random binary secret.\"\"\"\n",
    "    return torch.randint(0, 2, (batch_size, SECRET_LENGTH))   # [ B, N ]\n",
    "\n",
    "def bits_to_str(bits, sep=''):\n",
    "    \"\"\"Convert a bit tensor to a string representation.\"\"\"\n",
    "    return sep.join(str(b) for b in bits.tolist())\n",
    "\n",
    "def calculate_accuracy(preds, targets):\n",
    "    \"\"\"Calculate bitwise accuracy between predictions and targets.\"\"\"\n",
    "    correct = (preds == targets).sum().item()\n",
    "    total = targets.numel()\n",
    "    return correct / total\n",
    "\n",
    "def generate_carrier_prompt():\n",
    "    \"\"\"Generate a random carrier prompt.\"\"\"\n",
    "    subject = np.random.choice(INPUT_SUBJECTS)\n",
    "    return INPUT_PROMPT.format(subject)\n",
    "\n",
    "def generate_carrier_prompts():\n",
    "    \"\"\"Generate the full training set of carrier prompts.\"\"\"\n",
    "    return [ INPUT_PROMPT.format(subject) for subject in INPUT_SUBJECTS ]\n",
    "\n",
    "def build_generator_prompt(carrier, secret_bits = None):\n",
    "    \"\"\"Build the prompt for the generator model, optionally injecting the secret in the system prompt.\"\"\"\n",
    "    messages = [ ]\n",
    "\n",
    "    if INJECT_SECRET_IN_SYSTEM_PROMPT and secret_bits is not None:\n",
    "        messages.append(\n",
    "            {'role': 'system', 'content': f'You are a helpful assistant who always encodes these bits in their answer: {bits_to_str(secret_bits.flatten(), \" \")}. '}\n",
    "        )\n",
    "\n",
    "    messages.append({'role': 'user', 'content': carrier })\n",
    "\n",
    "    return generator_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "def kl_semantic_loss(stego_logits, base_logits):\n",
    "    \"\"\"Compute KL divergence between stego and base logits as semantic loss.\"\"\"\n",
    "    p = F.log_softmax(stego_logits, dim=-1)\n",
    "    q = F.softmax(base_logits, dim=-1)\n",
    "\n",
    "    return F.kl_div(p, q, reduction=\"batchmean\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode(carrier, secret_bits):\n",
    "    \"\"\"End-to-end encoding of secret bits into generated stego text based on the carrier prompt.\"\"\"\n",
    "    prompt = build_generator_prompt(carrier, secret_bits)\n",
    "    inputs = generator_tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    input_embeddings = encoder.generate_input_embeddings(inputs.input_ids.to(device), secret_bits)\n",
    "    past_kv = None\n",
    "\n",
    "    # Generate N tokens, store ids for each token\n",
    "    selected_tokens = [ ]\n",
    "    for t in range(STEGO_LENGTH):   \n",
    "        stego_logits, original_logits, token_probs, past_kv = encoder(\n",
    "            secret_bit = secret_bits[:, t],\n",
    "            inputs_embeds=input_embeddings,\n",
    "            past_key_values=past_kv\n",
    "        )\n",
    "\n",
    "        # Select the top probability token\n",
    "        predictions = torch.argmax(token_probs, dim=-1)\n",
    "\n",
    "        # Convert the token to 'hard' embeddings again for the next iteration\n",
    "        embedding_layer = generator.get_input_embeddings()\n",
    "        input_embeddings = embedding_layer(predictions).unsqueeze(1) # (B, 1, D)\n",
    "        \n",
    "        selected_tokens.append(predictions[0])\n",
    "\n",
    "    return generator_tokenizer.decode(selected_tokens, skip_special_tokens=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode(stego_text):\n",
    "    \"\"\"End-to-end decoding of stego text to recover the secret bits.\"\"\"\n",
    "    encoded_token_ids = generator_tokenizer(stego_text, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    # Ensure correct length, discard extra tokens and pad if too short\n",
    "    encoded_token_ids = encoded_token_ids[:, :STEGO_LENGTH]\n",
    "    encoded_token_ids = F.pad(encoded_token_ids, (0, STEGO_LENGTH - encoded_token_ids.shape[1]), value=generator_tokenizer.pad_token_id)\n",
    "\n",
    "    encoded_token_probs = F.one_hot(encoded_token_ids, vocab_size).float().to(device)\n",
    "\n",
    "    out = decoder(\n",
    "        token_probs = encoded_token_probs,\n",
    "    )\n",
    "\n",
    "    logits = out[\"logits\"]      # (B, N, 1)\n",
    "    preds = (logits.squeeze(-1) > 0).long()\n",
    "    \n",
    "    return preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da514610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "Tell me a fact about art and culture.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "----------------\n",
      "Art and culture are two of the most fascinating aspects of human experience. In my\n",
      "----------------\n",
      "Original: 0110110101111111\n",
      "Decoded: 1111111111101011\n"
     ]
    }
   ],
   "source": [
    "# Test prompt generation and generation inference\n",
    "\n",
    "# Prepare carrier and secret into a prompt\n",
    "carrier = generate_carrier_prompt()\n",
    "secret = generate_secret().to(device)\n",
    "print(build_generator_prompt(carrier, secret))\n",
    "\n",
    "print('----------------')\n",
    "\n",
    "# Encode the secret into stego text\n",
    "stego = encode(carrier, secret)\n",
    "print(stego)\n",
    "\n",
    "print('----------------')\n",
    "\n",
    "# Decode the stego text\n",
    "decoded = decode(stego)\n",
    "print(f'Original: {bits_to_str(secret[0])}')\n",
    "print(f'Decoded: {bits_to_str(decoded)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4b4c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "\n",
    "def loss_schedule(epoch, max_epoch):\n",
    "    p = min(epoch / RECOVERY_LOSS_STEPS, 1.0)\n",
    "    alpha = RECOVERY_LOSS_ALPHA_START + (RECOVERY_LOSS_ALPHA_END - RECOVERY_LOSS_ALPHA_START) * p\n",
    "    return alpha, 1.0 - alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3090283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 27\n",
      "L_sem: 0.281 | L_rec: 0.650 | Acc: 0.625\n",
      "Lr: 0.056798 | w_rec: 0.855 | w_sem: 0.145\n",
      "\n",
      "Prompt: Tell me a fact about sports.\n",
      "Stego: A fact about sports is that sports are a sport that involves physical activity, competition\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     79\u001b[39m prompt = generate_carrier_prompt()\n\u001b[32m     80\u001b[39m secret_bits = generate_secret().to(device)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m stego_text = \u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecret_bits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m decoded_secret_bits = decode(stego_text)\n\u001b[32m     84\u001b[39m display.clear_output(wait=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mencode\u001b[39m\u001b[34m(carrier, secret_bits)\u001b[39m\n\u001b[32m     56\u001b[39m selected_tokens = [ ]\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(STEGO_LENGTH):   \n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     stego_logits, original_logits, token_probs, past_kv = \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43msecret_bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msecret_bits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Select the top probability token\u001b[39;00m\n\u001b[32m     65\u001b[39m     predictions = torch.argmax(token_probs, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mStegoEncoder.forward\u001b[39m\u001b[34m(self, secret_bit, temperature, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, secret_bit = \u001b[38;5;28;01mNone\u001b[39;00m, temperature=\u001b[32m1.0\u001b[39m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_lm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# prevent early stop\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# Select logits for the last token only (this is the next prediction)\u001b[39;00m\n\u001b[32m     53\u001b[39m     logits = out.logits[:, -\u001b[32m1\u001b[39m, :]   \u001b[38;5;66;03m# [B, V]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:459\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    441\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    444\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\utils\\generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:395\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    407\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    408\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    409\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:294\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    292\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    293\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    306\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:252\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation != \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    250\u001b[39m     attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    264\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:96\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.dtype != torch.bool:\n\u001b[32m     93\u001b[39m         \u001b[38;5;66;03m# Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\u001b[39;00m\n\u001b[32m     94\u001b[39m         attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msdpa_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# Collect all model parameters\n",
    "collect_params = lambda model: [ p for p in model.parameters() if p.requires_grad ]\n",
    "\n",
    "parameters = [ ]\n",
    "parameters += collect_params(encoder)\n",
    "parameters += collect_params(decoder)\n",
    "\n",
    "# Initialize optimizer to guide training\n",
    "optimizer = torch.optim.AdamW(parameters, lr=LR_INITIAL)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=LR_GAMMA)\n",
    "\n",
    "# Prepare prompts\n",
    "print('Preparing training prompts...')\n",
    "\n",
    "carrier_prompts = generate_carrier_prompts()\n",
    "generator_prompts = [ build_generator_prompt(carrier) for carrier in carrier_prompts ]\n",
    "generator_inputs = generator_tokenizer(generator_prompts, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "\n",
    "print('Generator inputs shape:', generator_inputs.shape)\n",
    "exit()\n",
    "\n",
    "# Training loop for fixed number of steps\n",
    "print('Starting training...')\n",
    "for step in range(MAX_EPOCH):\n",
    "    secret_bits = generate_secret(generator_inputs.shape[0]).to(device)\n",
    "\n",
    "    # Prepare embeddings for first step by adding secret prefix to token embeddings\n",
    "    # NOTE: when not using prefix embeddings, these are constant and could be pre-generated\n",
    "    soft_embeds = encoder.generate_input_embeddings(generator_inputs, secret_bits)\n",
    "    past_kv = None\n",
    "\n",
    "    # Generate N tokens, store probabilities for each token\n",
    "    generated_token_probs = []\n",
    "\n",
    "    # Initialize semantic loss, which we accumulate per generated token\n",
    "    semantic_loss = 0.0\n",
    "\n",
    "    for t in range(STEGO_LENGTH): \n",
    "        stego_logits, original_logits, token_probs, past_kv = encoder(\n",
    "            secret_bit=secret_bits[:, t],\n",
    "            inputs_embeds=soft_embeds,\n",
    "            past_key_values=past_kv\n",
    "        )\n",
    "\n",
    "        # Calculate semantic loss compared to base logits\n",
    "        semantic_loss += kl_semantic_loss(stego_logits, original_logits)\n",
    "\n",
    "        # Prepare soft embeddings for next step\n",
    "        soft_embeds = encoder.soft_embeddings(token_probs)  # [ B, D ]\n",
    "\n",
    "        # Store all generated token probabilities\n",
    "        generated_token_probs.append(token_probs)\n",
    "\n",
    "        # Take soft embeds for next step\n",
    "        soft_embeds = soft_embeds.unsqueeze(1)\n",
    "\n",
    "    # Stack probabilities for each token to [ B, N, D ]\n",
    "    encoder_outputs_probs = torch.stack(generated_token_probs, dim=1).to(device)\n",
    "\n",
    "    # Calculate recovery loss based on decoding success\n",
    "    recovery_loss = decoder(\n",
    "        token_probs=encoder_outputs_probs, \n",
    "        labels=secret_bits.to(device)\n",
    "    )['loss']\n",
    "\n",
    "    # Combine loss based on loss weighting schedule\n",
    "    w_recovery, w_semantic = loss_schedule(step, MAX_EPOCH)\n",
    "    loss = w_recovery * recovery_loss + w_semantic * semantic_loss\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "\n",
    "    if step % 1 == 0:\n",
    "        prompt = generate_carrier_prompt()\n",
    "        secret_bits = generate_secret().to(device)\n",
    "        stego_text = encode(prompt, secret_bits)\n",
    "        decoded_secret_bits = decode(stego_text)\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        print(f\"Step {step}\")\n",
    "        print(f\"L_sem: {semantic_loss.item():.3f} | L_rec: {recovery_loss.item():.3f} | Acc: {calculate_accuracy(decoded_secret_bits, secret_bits):.3f}\")\n",
    "        print(f'Lr: {scheduler.get_last_lr()[0]:.6f} | w_rec: {w_recovery:.3f} | w_sem: {w_semantic:.3f}')\n",
    "        print()\n",
    "        print(f'Prompt: {prompt}')\n",
    "        print(f'Stego: {stego_text}')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb69447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final end-to-end test\n",
    "\n",
    "carrier = generate_carrier_prompt()\n",
    "secret = generate_secret().to(device)\n",
    "\n",
    "stego_text = encode(carrier, secret)\n",
    "recovered_secret = decode(stego_text)\n",
    "\n",
    "print(\"Stego text:\")\n",
    "print(stego_text)\n",
    "print()\n",
    "print(\"Original secret:\", bits_to_str(secret[0]))\n",
    "print(\"Recovered secret:\", bits_to_str(recovered_secret))\n",
    "print(f\"Accuracy: {100 * calculate_accuracy(recovered_secret, secret):.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stega",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
