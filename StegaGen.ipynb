{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52220c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "import nltk\n",
    "import evaluate\n",
    "import torch\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf2aa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA on NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Determine what device to use\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    device_name = torch.cuda.get_device_name(device)\n",
    "\n",
    "    print(f'CUDA on {device_name}')\n",
    "else:\n",
    "    print(f'CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd88ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "\n",
    "SECRET_LENGTH = 16\n",
    "TOKENS_PER_BIT = 1\n",
    "SKIPPED_OUTPUT_TOKENS = 4   \t\t\t# Number of initial output tokens to skip (seem to be constant for SmolLM)\n",
    "PREFIX_LENGTH = 0\t\t\t\t\t\t# Number of tokens in which the secret is encoded before the input prompt. 0 to disable\n",
    "INJECT_SECRET_IN_SYSTEM_PROMPT = True \t# Whether to insert the secret as binary text in the system prompt\n",
    "\n",
    "SHIFT_EMBEDS = False\t\t\t\t\t# A debug hack to trivially encode tokens by shifting embedding bits. Useful to test decoder\n",
    "MASK_TOKENS = True\t\t\t\t\t\t# Whether to mask a subset of tokens basd on the encoded secret bit. A simple form of encoding\n",
    "\n",
    "STEGO_LENGTH = SECRET_LENGTH * TOKENS_PER_BIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9584927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup previous run\n",
    "# This allows rerunning the entire notebook without memory leaks\n",
    "\n",
    "import gc\n",
    "\n",
    "# Delete models, clean up memory\n",
    "\n",
    "if 'generator_tokenizer' in globals():\n",
    "    del generator_tokenizer\n",
    "    \n",
    "if 'decoder_tokenizer' in globals():\n",
    "    del decoder_tokenizer\n",
    "\n",
    "if 'generator' in globals():\n",
    "    del generator\n",
    "    \n",
    "if 'decoder' in globals():\n",
    "    del decoder\n",
    "    \n",
    "if 'semantic_anchor' in globals():\n",
    "    del semantic_anchor\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch._C._cuda_clearCublasWorkspaces()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef47b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.0 MB; Reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Check if everything's cleaned\n",
    "allocated = torch.cuda.memory_allocated() / (1024 * 1024)\n",
    "reserved = torch.cuda.memory_reserved() / (1024 * 1024)\n",
    "\n",
    "print(f'Allocated: {allocated:.1f} MB; Reserved: {reserved:.1f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbad1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StegoEmbeddingDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size=768, output_size=16, pool_size=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "        config = BertConfig(\n",
    "            hidden_size=hidden_size,\n",
    "            num_hidden_layers=4,\n",
    "            num_attention_heads=8,\n",
    "            intermediate_size=16 * hidden_size\n",
    "        )\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        # Token-wise binary classification head\n",
    "        # NOTE: 1 layer seems to work fine for now\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size * pool_size, 1),\n",
    "            # Note: Sigmoid() seems to strongly decrease decoder effectiveness?\n",
    "            # nn.GELU(),\n",
    "            # nn.Linear(256, 128),\n",
    "            # nn.GELU(),\n",
    "            # nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs_embeds, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        inputs_embeds: (B, N, D)\n",
    "        attention_mask: (B, N) optional\n",
    "        labels: (B, N) optional\n",
    "        \"\"\"\n",
    "\n",
    "        # Run input embeddings through BERT model\n",
    "        outputs = self.bert(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state # (B, N, D)\n",
    "\n",
    "        # Reshape embeddings per token to stack all embeddings for a bit pool\n",
    "        assert hidden_states.shape[1] == self.output_size * self.pool_size, f'Input tokens size error: {hidden_states.shape[1]}. Expected: {self.output_size * self.pool_size}'\n",
    "        assert hidden_states.shape[2] == self.hidden_size, f' Input embedding size error: {hidden_states.shape[2]}. Expected: {self.hidden_size}'\n",
    "        classifier_input = hidden_states.reshape(-1, self.output_size, self.pool_size * self.hidden_size)  # (B, T, P * D)\n",
    "        \n",
    "        # Perform classification to get logits for each bit\n",
    "        logits = self.classifier(classifier_input)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fn(logits.squeeze(-1), labels.float())\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3e9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SecretPrefixEncoder(nn.Module):\n",
    "    def __init__(self, secret_length: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.secret_length = secret_length\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(1, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, bits: torch.Tensor):\n",
    "        \"\"\"\n",
    "        bits: [ B, S ] in {0,1}\n",
    "        returns: [ B, S, D ]\n",
    "        \"\"\"\n",
    "        emb = self.projection(bits.reshape(-1, self.secret_length, 1).float())\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a15a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator embedding dimensions: 576, vocabulary size: 49152\n"
     ]
    }
   ],
   "source": [
    "# Create models\n",
    "\n",
    "#GENERATOR_MODEL = \"Qwen/Qwen3-0.6B\"\n",
    "#GENERATOR_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "#GENERATOR_MODEL = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "GENERATOR_MODEL = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL)\n",
    "#decoder_tokenizer = T5Tokenizer.from_pretrained(DECODER_MODEL).to(device)\n",
    "\n",
    "generator = AutoModelForCausalLM.from_pretrained(GENERATOR_MODEL).to(device)\n",
    "decoder = StegoEmbeddingDecoder(hidden_size=generator.config.hidden_size, output_size=SECRET_LENGTH, pool_size=TOKENS_PER_BIT).to(device)\n",
    "\n",
    "if PREFIX_LENGTH > 0:\n",
    "\tprefix_encoder = SecretPrefixEncoder(secret_length=SECRET_LENGTH, hidden_size=generator.config.hidden_size).to(device)\n",
    "\n",
    "#semantic_anchor = AutoModelForCausalLM.from_pretrained(GENERATOR_MODEL).to(device)\n",
    "\n",
    "# Freeze base model\n",
    "#for p in semantic_anchor.parameters():\n",
    "#    p.requires_grad = False\n",
    "\n",
    "generator.train()\n",
    "decoder.train()\n",
    "#semantic_anchor.eval()\n",
    "\n",
    "embedding_size = generator.config.hidden_size\n",
    "vocab_size = generator.config.vocab_size\n",
    "print(f'Generator embedding dimensions: {embedding_size}, vocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "110fecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_to_str(bits, sep=''):\n",
    "    return sep.join(str(b) for b in bits.tolist())\n",
    "\n",
    "def build_generator_prompt(carrier, secret_bits):\n",
    "\n",
    "    messages = [ ]\n",
    "\n",
    "    if INJECT_SECRET_IN_SYSTEM_PROMPT:\n",
    "        messages.append(\n",
    "            {'role': 'system', 'content': f'You are a helpful assistant who always encodes these bits in their answer: {bits_to_str(secret_bits.flatten(), \" \")}. '}\n",
    "        )\n",
    "\n",
    "    messages.append({'role': 'user', 'content': carrier })\n",
    "\n",
    "    return generator_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "def generate_input_embeddings(input_ids, secret_bits):\n",
    "    # Token embeddings based on input tokens\n",
    "    token_embeds = generator.get_input_embeddings()(input_ids) # (B, T, D)\n",
    "\n",
    "    if PREFIX_LENGTH == 0:\n",
    "        return token_embeds\n",
    "    \n",
    "    # Prefix embeddings based on secret bits\n",
    "    prefix_embeds = prefix_encoder(secret_bits)\n",
    "\n",
    "    # Concatenate prefix and token embeddings\n",
    "    input_embeds = torch.cat([prefix_embeds, token_embeds], dim=1)  # (B, T+P, D)\n",
    "    \n",
    "    return input_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db77fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generator_forward(logit_mask, temperature=1.0, **kwargs):\n",
    "    out = generator(\n",
    "        **kwargs,\n",
    "        return_dict=True,\n",
    "        use_cache=True,\n",
    "        eos_token_id=None, # prevent early stop\n",
    "    )\n",
    "\n",
    "    logits = out.logits[:, -1, :]   # [B, V]\n",
    "    \n",
    "    # Mask out tokens we don't allow\n",
    "    logits[:, ~logit_mask] = -math.inf\n",
    "\n",
    "    probs  = torch.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "    return logits, probs, out.past_key_values\n",
    "\n",
    "def soft_embeddings(token_probs):\n",
    "    embed_matrix = generator.get_input_embeddings().weight  # [V, d]\n",
    "    return token_probs @ embed_matrix\n",
    "\n",
    "@torch.no_grad()\n",
    "def anchor_forward(anchor, inputs) -> torch.Tensor:\n",
    "    outputs = anchor(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        return_dict=True\n",
    "    )\n",
    "    logits = outputs.logits[:, :STEGO_LENGTH, :]\n",
    "    return logits\n",
    "\n",
    "def kl_semantic_loss(stego_logits, base_logits):\n",
    "    p = F.log_softmax(stego_logits, dim=-1)\n",
    "    q = F.softmax(base_logits, dim=-1)\n",
    "\n",
    "    # Masked?\n",
    "    # kl = F.kl_div(log_p, q, reduction=\"none\")  # [B, T, V]\n",
    "    # kl = kl.sum(dim=-1)                        # [B, T]\n",
    "\n",
    "    # # Mask padding tokens\n",
    "    # kl = kl * attention_mask\n",
    "\n",
    "    # return kl.sum() / attention_mask.sum()\n",
    "\n",
    "    return F.kl_div(p, q, reduction=\"batchmean\")\n",
    "\n",
    "def decoder_forward(soft_embeds, secret_bits):\n",
    "    out = decoder(inputs_embeds=soft_embeds, labels=secret_bits)\n",
    "\n",
    "    return out['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f60c8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode(carrier, secret_bits):\n",
    "    prompt = build_generator_prompt(carrier, secret_bits)\n",
    "    inputs = generator_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_embeddings = generate_input_embeddings(inputs.input_ids.to(device), secret_bits)\n",
    "\n",
    "    ids = generator.generate(\n",
    "        inputs_embeds=input_embeddings,\n",
    "        max_new_tokens=STEGO_LENGTH + SKIPPED_OUTPUT_TOKENS, # First X tokens seem to be constant part of the LMs output\n",
    "        eos_token_id=None, # prevent early stop\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    selected_ids = ids[0, SKIPPED_OUTPUT_TOKENS:]\n",
    "\n",
    "    assert selected_ids.shape[0] == STEGO_LENGTH, f'Generated stego length error: {selected_ids.shape[0]}. Expected: {STEGO_LENGTH}'\n",
    "\n",
    "    return generator_tokenizer.decode(selected_ids, skip_special_tokens=True)\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode(stego_text):\n",
    "    encoded_token_ids = generator_tokenizer(stego_text, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    # Ensure correct length, discard extra tokens and pad if too short\n",
    "    encoded_token_ids = encoded_token_ids[:, :STEGO_LENGTH]\n",
    "    encoded_token_ids = F.pad(encoded_token_ids, (0, STEGO_LENGTH - encoded_token_ids.shape[1]), value=generator_tokenizer.pad_token_id)\n",
    "\n",
    "    # Encoder embedding matrix\n",
    "    embedding_layer = generator.get_input_embeddings()\n",
    "\n",
    "    # (B, N, D)\n",
    "    encoded_embeds = embedding_layer(encoded_token_ids.to(device))\n",
    "\n",
    "    #attention_mask = torch.ones(encoded_embeds.shape[:2])\n",
    "\n",
    "    out = decoder(\n",
    "        inputs_embeds=encoded_embeds\n",
    "        #attention_mask=attention_mask,\n",
    "    )\n",
    "\n",
    "    logits = out[\"logits\"]      # (B, N, 1)\n",
    "    preds = (logits.squeeze(-1) > 0).long()\n",
    "    \n",
    "    return preds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94de9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_secret():\n",
    "    return torch.randint(0, 2, (1, SECRET_LENGTH))   # [ B, N ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da514610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant who always encodes these bits in their answer: 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1. <|im_end|>\n",
      "<|im_start|>user\n",
      "Write a short paragraph explaining how neural networks learn.<|im_end|>\n",
      "\n",
      "----------------\n",
      "Neural networks learn by transforming raw data into a set of patterns that can be\n"
     ]
    }
   ],
   "source": [
    "# Test prompt generation and generation inference\n",
    "\n",
    "carrier = \"Write a short paragraph explaining how neural networks learn.\"\n",
    "secret = generate_secret().to(device)\n",
    "\n",
    "print(build_generator_prompt(carrier, secret))\n",
    "print('----------------')\n",
    "print(encode(carrier, secret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3090283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | L_sem: 0.000 | L_rec: 0.513\n",
      "Secret:  0010000000101110\n",
      "Decoded: 0001101111100110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 1 | L_sem: 0.000 | L_rec: 0.242\n",
      "Secret:  1010110000101111\n",
      "Decoded: 1001101111100110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 2 | L_sem: 0.000 | L_rec: 0.247\n",
      "Secret:  1100101000010011\n",
      "Decoded: 1001101111111010\n",
      "Stego:   Neural networks learn by transforming the input data into a set of weights and biases\n",
      "\n",
      "Step 3 | L_sem: 0.000 | L_rec: 0.034\n",
      "Secret:  1001110000010111\n",
      "Decoded: 1101101111100110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 4 | L_sem: 0.000 | L_rec: 0.060\n",
      "Secret:  1001011001010000\n",
      "Decoded: 1011101111100110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 5 | L_sem: 0.000 | L_rec: 0.027\n",
      "Secret:  1011001011110100\n",
      "Decoded: 1111101111111111\n",
      "Stego:   Neural networks learn by transforming data into a mathematical representation that can be used to\n",
      "\n",
      "Step 6 | L_sem: 0.000 | L_rec: 0.025\n",
      "Secret:  0000011110110001\n",
      "Decoded: 1111101111101110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 7 | L_sem: 0.000 | L_rec: 0.004\n",
      "Secret:  1011010000100011\n",
      "Decoded: 1111111111101110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 8 | L_sem: 0.000 | L_rec: 0.020\n",
      "Secret:  1100000111001100\n",
      "Decoded: 1111101111111110\n",
      "Stego:   Neural networks learn by transforming the input data into a set of weights and biases\n",
      "\n",
      "Step 9 | L_sem: 0.000 | L_rec: 0.003\n",
      "Secret:  1011011001100110\n",
      "Decoded: 1111101111100110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 10 | L_sem: 0.000 | L_rec: 0.011\n",
      "Secret:  0011110100010100\n",
      "Decoded: 1111101111101110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 11 | L_sem: 0.000 | L_rec: 0.007\n",
      "Secret:  0000010011001101\n",
      "Decoded: 1111101111100110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 12 | L_sem: 0.000 | L_rec: 0.002\n",
      "Secret:  0110101100110010\n",
      "Decoded: 1111101111100110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 13 | L_sem: 0.000 | L_rec: 0.006\n",
      "Secret:  1101101011100111\n",
      "Decoded: 1111101111111110\n",
      "Stego:   Neural networks learn by transforming the input data into a set of weights and biases\n",
      "\n",
      "Step 14 | L_sem: 0.000 | L_rec: 0.002\n",
      "Secret:  0001111010001010\n",
      "Decoded: 1111101111100110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 15 | L_sem: 0.000 | L_rec: 0.002\n",
      "Secret:  1101010111110001\n",
      "Decoded: 1111101111111110\n",
      "Stego:   Neural networks learn by transforming the input data into a set of weights and biases\n",
      "\n",
      "Step 16 | L_sem: 0.000 | L_rec: 0.001\n",
      "Secret:  0101011100011000\n",
      "Decoded: 1111101111101110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 17 | L_sem: 0.000 | L_rec: 0.002\n",
      "Secret:  0011110100010100\n",
      "Decoded: 1111101111100110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 18 | L_sem: 0.000 | L_rec: 0.000\n",
      "Secret:  0110111111101110\n",
      "Decoded: 1111101111101110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 19 | L_sem: 0.000 | L_rec: 0.001\n",
      "Secret:  0111001101011100\n",
      "Decoded: 1111101111101110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 20 | L_sem: 0.000 | L_rec: 0.003\n",
      "Secret:  1110011000011100\n",
      "Decoded: 1111101111111110\n",
      "Stego:   Neural networks learn by transforming the input data into a set of weights and biases\n",
      "\n",
      "Step 21 | L_sem: 0.000 | L_rec: 0.003\n",
      "Secret:  0100101100111100\n",
      "Decoded: 1111101111100110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 22 | L_sem: 0.000 | L_rec: 0.001\n",
      "Secret:  0011110110010011\n",
      "Decoded: 1111101111101110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 23 | L_sem: 0.000 | L_rec: 0.000\n",
      "Secret:  0001110101011100\n",
      "Decoded: 1111101111101110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 24 | L_sem: 0.000 | L_rec: 0.001\n",
      "Secret:  0000000000111101\n",
      "Decoded: 1111101111101110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 25 | L_sem: 0.000 | L_rec: 0.001\n",
      "Secret:  1000001101000001\n",
      "Decoded: 1111101111100110\n",
      "Stego:   Neural networks learn by transforming data into a set of interconnected nodes or \"neurons\n",
      "\n",
      "Step 26 | L_sem: 0.000 | L_rec: 0.001\n",
      "Secret:  1100111111001101\n",
      "Decoded: 1111101111111110\n",
      "Stego:   Neural networks learn by transforming the input data into a set of weights and biases\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     34\u001b[39m         end = vocab_size \u001b[38;5;28;01mif\u001b[39;00m bit == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m vocab_size // \u001b[32m2\u001b[39m\n\u001b[32m     36\u001b[39m         token_mask[start:end] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m stego_logits, token_probs, past_kv = \u001b[43mgenerator_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43msoft_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m soft_embeds = soft_embeddings(token_probs)  \u001b[38;5;66;03m# [ B, D ]\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Only save embeds after the first X tokens\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mgenerator_forward\u001b[39m\u001b[34m(logit_mask, temperature, **kwargs)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerator_forward\u001b[39m(logit_mask, temperature=\u001b[32m1.0\u001b[39m, **kwargs):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     out = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# prevent early stop\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     logits = out.logits[:, -\u001b[32m1\u001b[39m, :]   \u001b[38;5;66;03m# [B, V]\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Mask out tokens we don't allow\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:459\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    441\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    444\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\utils\\generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:395\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    407\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    408\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    409\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:294\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    292\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    293\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    306\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:252\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation != \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    250\u001b[39m     attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    264\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\miniconda3\\envs\\stega\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:96\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.dtype != torch.bool:\n\u001b[32m     93\u001b[39m         \u001b[38;5;66;03m# Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\u001b[39;00m\n\u001b[32m     94\u001b[39m         attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msdpa_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "\n",
    "# Collect all model parameters\n",
    "parameters = list(generator.parameters()) + list(decoder.parameters())\n",
    "\n",
    "if PREFIX_LENGTH > 0:\n",
    "    parameters += list(prefix_encoder.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW(parameters, lr=3e-5)\n",
    "\n",
    "for step in range(1000):\n",
    "    carrier_prompt = \"Explain how neural networks learn.\"\n",
    "    secret_bits = generate_secret().to(device)\n",
    "\n",
    "    prompt = build_generator_prompt(carrier_prompt, secret_bits)\n",
    "    generator_inputs = generator_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # Prepare embeddings for first step by adding secret prefix to token embeddings\n",
    "    soft_embeds = generate_input_embeddings(generator_inputs, secret_bits)\n",
    "    past_kv = None\n",
    "\n",
    "    # Generate N tokens, store embeddings for each token\n",
    "    generated_embeds = []\n",
    "    for t in range(STEGO_LENGTH + SKIPPED_OUTPUT_TOKENS):   \n",
    "\n",
    "        # Create a mask to mask out subsets of tokens\n",
    "        token_mask = torch.full((vocab_size, ), True)\n",
    "\n",
    "        if t >= SKIPPED_OUTPUT_TOKENS:\n",
    "            if MASK_TOKENS:\n",
    "                bit = secret_bits[0, t - SKIPPED_OUTPUT_TOKENS]\n",
    "                start = 0 if bit == 0 else vocab_size // 2\n",
    "                end = vocab_size if bit == 1 else vocab_size // 2\n",
    "\n",
    "                token_mask[start:end] = False\n",
    "\n",
    "        stego_logits, token_probs, past_kv = generator_forward(\n",
    "            token_mask,\n",
    "            inputs_embeds=soft_embeds,\n",
    "            past_key_values=past_kv\n",
    "        )\n",
    "\n",
    "        soft_embeds = soft_embeddings(token_probs)  # [ B, D ]\n",
    "\n",
    "        # Only save embeds after the first X tokens\n",
    "        if t >= SKIPPED_OUTPUT_TOKENS:\n",
    "            if SHIFT_EMBEDS:\n",
    "                # HACK: Modulate embeddings based on secret bit to test decoder\n",
    "                shifted_embeds = torch.mul(soft_embeds, bit * 2 - 1)\n",
    "            else:\n",
    "                shifted_embeds = soft_embeds\n",
    "\n",
    "            generated_embeds.append(shifted_embeds)\n",
    "\n",
    "        # Take soft embeds for next step\n",
    "        soft_embeds = soft_embeds.unsqueeze(1)\n",
    "\n",
    "    #print(f'Number of generated embeddings: {len(generated_embeds)}, skipping first {SKIPPED_OUTPUT_TOKENS}...')\n",
    "\n",
    "    # Stack embeddings for each token to [ B, N, D ]\n",
    "    encoder_outputs_embeds = torch.stack(generated_embeds, dim=1).to(device)\n",
    "\n",
    "    #print(f'Encoder outputs embeddings shape: {encoder_outputs_embeds.shape}')\n",
    "\n",
    "    # Calculate recovery loss based on decoding success\n",
    "    recovery_loss = decoder_forward(encoder_outputs_embeds, secret_bits.to(device))\n",
    "\n",
    "    # Calculate semantic loss compared to base model\n",
    "    #base_logits = anchor_forward(semantic_anchor, generator_inputs)\n",
    "    #semantic_loss = kl_semantic_loss(stego_logits, base_logits)\n",
    "    semantic_loss = np.zeros(1)\n",
    "\n",
    "    # Combine loss\n",
    "    loss = recovery_loss\n",
    "    #loss = 1.0 * recovery_loss + 0.0 * semantic_loss\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 1 == 0:\n",
    "        stego_text = encode(carrier_prompt, secret_bits)\n",
    "        secret_str = bits_to_str(secret_bits.flatten())\n",
    "        decoded_secret_bits = decode(stego_text)\n",
    "        decoded_secret_str = bits_to_str(decoded_secret_bits)\n",
    "\n",
    "        print(f\"Step {step} | L_sem: {semantic_loss.item():.3f} | L_rec: {recovery_loss.item():.3f}\")\n",
    "        print(f'Secret:  {secret_str}')\n",
    "        print(f'Decoded: {decoded_secret_str}')\n",
    "        print(f'Stego:   {stego_text}')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb69447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final end-to-end test\n",
    "\n",
    "carrier = \"Write a short paragraph explaining how neural networks learn.\"\n",
    "secret = generate_secret().to(device)\n",
    "\n",
    "stego_text = encode(carrier, secret)\n",
    "recovered_secret = decode(stego_text)\n",
    "\n",
    "print(\"Stego text:\")\n",
    "print(stego_text)\n",
    "print()\n",
    "print(\"Original secret:\", bits_to_str(secret))\n",
    "print(\"Recovered secret:\", bits_to_str(recovered_secret))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stega",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
